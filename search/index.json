[{"content":"有状态的水平扩展 现实中负载增强后需要更强的处理能力，一般来说会分为两种扩展，水平扩展和垂直扩展。垂直扩展通常就是往一台机器上加更多的内存和CPU核心，成本较高，容错能力较差。水平扩展则是一台机器处理任务变成多台机器一起处理任务，相互协调通信全部运行在以太网上。无数据状态的水平扩展比较简单，无论是添加还是删除都不会给应用程序造成影响，而对于有数据状态的水平扩展(例如数据库)，通常来说会有两种方式：\n复制 分区 复制 复制通常为了解决几个问题：贴近用户，降低访问延迟、高可用、读吞吐量。 复制中的所有挑战都是为了解决持续更改中的复制问题。 其中流行的三种复制方法有：\n主从复制 多主节点复制 无主节点复制 主从复制 每个保存数据库完整数据集的节点称之为副本。复制设计中分为两种，一种是同步复制一种是异步复制。\n同步复制好处是，从节点可以确保完成和主节点的更新同步。坏处是每次更改都需要等待从节点确认，最差情况会阻塞当前操作。\n其中有一种设计是半同步，只有一个节点是同步复制的，如果同步的从节点变得不可用，就可以提升异步的从节点为同步。这样可以确保至少有两个节点拥有最新的副本。\n如果是全异步模式，很有可能会在主节点还没来及的发送写请求却宕机，但是客户端已经确认了。这样无法保证数据的持久化。\n如果加入了新的从节点，这时候主节点通常还是会对外保持服务，源源不断产生写请求。通常来说数据库系统会对数据产生一个快照，避免锁住整个数据库(不可接受)，然后拷贝快照到从节点。接下来快照时间后所有产生数据更改日志也会发给从节点，这样被称为追赶。\n接下来讨论最关键的高可用(宕机)环节。\n节点失效 从节点失效：追赶式恢复，根据副本的复制日志，可以知道故障前最后处理的事务，然后去主节点拿到这个事务后所有的数据变更。\n主节点失效：节点切换。分为三个步骤，确认主节点失效，选举新的主节点，重新配置系统生效主节点。最后一步中客户端需要将请求发送给新的主节点，如果旧的主节点重新上线，可能还自认为是主节点（脑裂），系统需要确保旧的主节点降级为从节点。 如果是异步复制，就有可能导致数据丢失，如果失效前的数据没有被复制出去，重新上线后可能会给其他从节点发送同步请求，这样的话只能丢弃旧主节点上的数据。 如果使用丢失数据的方式来解决数据复制不同步的问题，首先会破坏数据持久化，其次，如果依赖了Redis缓存等外部组件，可能会出现数据不一致，导致数据错误或泄漏。\n复制日志 基于语句的复制，所有操作日志发给从节点，但是有一些问题，例如NOW获取当前时间，在从节点就会产生不同的值。或者是使用了自增列，就必须顺序执行。 基于WAL预写日志的复制，所有对数据库写入的字节序列都写入日志，然后发送给从节点重放。但是这种数据过于底层，必须与存储引擎紧密结合才能生效。 基于行的逻辑日志的复制，逻辑日志指一系列记录描述对行级别的写请求，例如行插入、行删除、行更新。MySQL的binlog就是其中一种。\n复制滞后(最终一致性) 完全同步复制的系统在现实中非常不可靠，如果其中只要一个节点发生故障整个系统就不能写入了。所以说只能用异步复制，但是使用异步复制就会导致客户端访问从节点会读到过期的消息。 接下来讨论复制滞后的问题与一些解决思路。\n","date":"2023-12-24T13:38:43Z","image":"https://kinokosu3.github.io/p/ddia%E7%B3%BB%E5%88%971%EF%B8%8F%E2%83%A3--%E5%A4%8D%E5%88%B6%E7%90%86%E8%AE%BA/cover_hu9cf6b63927dd4e5f6bed850b07c5d714_402406_120x120_fill_box_smart1_3.png","permalink":"https://kinokosu3.github.io/p/ddia%E7%B3%BB%E5%88%971%EF%B8%8F%E2%83%A3--%E5%A4%8D%E5%88%B6%E7%90%86%E8%AE%BA/","title":"DDIA系列1️⃣--复制理论"},{"content":"题目解读 根据题目可得下标大于或等于值获取一分，整个数组的值步数增大一步，数组的值就会抽象的向左移动，然后向左移动的值，填充到右边。然后根据规则计算分数得出哪个步数的分最多，返回步数。\n思路 通过对比每次步数之后的变化，发现下标小于值的左移后依旧没有得分，下标大于值的左移后，直到碰到与自己相等的下标后才会失去值。共得出两种情况会出现分数变化，值从自己相等的下标位置左移、首位移动到末尾(因为值小于长度，所以值小于等于下标nums[i] \u0026lt; nums.length，所以移动到末尾的值必定小于或等于下标)。然后我们可以得出状态转移公式：当前分数 = 上一次分数 - 上一次有几个值与下标相等 + 1(首位移动到末尾的分数变化);\n可以提前计算出每一步里有多少个值与下标相等的值，假如一个值现小于或等于下标，要将该值移动到相等下标的位置所需步数 下标 - 值。假如一个值大于下标，则该下标位置会在右边，所需步数首先 下标 + 数组长度 - 值。\n然后通过遍历，筛选出最大的步数。\nfunc bestRotation(nums []int) int { // index大于值或者等于记一分 // 多个同分，选最小 n := len(nums) val := 0 // 计数当前已经得分的数 for i:=0;i \u0026lt; n;i++ { if nums[i] \u0026lt;= i{ val++ } } steps := make([]int, n,n) for i :=0;i\u0026lt;n;i++{ if nums[i] \u0026lt;= i{ // 得分的时候， steps[i - nums[i]]++ }else{ steps[i + n - nums[i]]++ } } ans := 0 maxval := val for i :=1;i \u0026lt; n;i++{ val = val - steps[i-1] + 1; if val \u0026gt; maxval{ ans = i maxval = val } } return ans } ","date":"2022-03-09T14:26:37Z","permalink":"https://kinokosu3.github.io/p/798.%E5%BE%97%E5%88%86%E6%9C%80%E9%AB%98%E7%9A%84%E6%9C%80%E5%B0%8F%E8%BD%AE%E8%B0%83/","title":"798.得分最高的最小轮调"},{"content":"准备工作 由于线上的小鸡性能过于赢弱，所以并不会真正的直接上k8s，而是使用性能较差设备用的k3s上，基本上的功能都有，体验与k8s并没有什么不同\n安装K3s 可以直接使用官网的一键安装：\ncurl -sfL https://get.k3s.io | sh - 也可以直接去官方github下载二进制文件，然后转移到系统的/usr/local/bin，然后修改权限chmod 755 k3s\n然后设置变量，跳过脚本的下载\nexport INSTALL_K3S_SKIP_DOWNLOAD=true 然后将get.k3s.io的安装脚本执行。\n安装Docker docker安装很简单，几句命令一键安装即可\n在k3s里运行jenkins 将jenkins作为一个pod在k3s里运行\ndeployment和service的yaml文件如下：\napiVersion: v1 kind: Service metadata: name: jenkins-service labels: app: jenkins spec: type: NodePort selector: app: jenkins ports: - protocol: TCP nodePort: 30080 port: 8080 name: web targetPort: 8080 - protocol: TCP port: 50000 name: agent targetPort: 50000 --- apiVersion: apps/v1 kind: Deployment metadata: name: jenkins spec: replicas: 1 selector: matchLabels: app: jenkins template: metadata: labels: app: jenkins spec: containers: - image: jenkinsci/blueocean:latest name: jenkins-container ports: - containerPort: 8080 name: jenkins-8080 protocol: TCP - containerPort: 50000 name: jenkins-50000 protocol: TCP volumeMounts: - name: jenkins-persistent-data mountPath: /var/jenkins_home volumes: - name: jenkins-persistent-data persistentVolumeClaim: claimName: jenkins-pv-data-claim pv和pvc的文件如下：\napiVersion: v1 kind: PersistentVolume metadata: name: jenkins-pv-data labels: type: local spec: storageClassName: jenkins-data capacity: storage: 512Mi accessModes: - ReadWriteOnce hostPath: path: \u0026#34;/your_path/jenkins_home\u0026#34; --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jenkins-pv-data-claim spec: storageClassName: jenkins-data accessModes: - ReadWriteOnce resources: requests: storage: 512Mi 这里的镜像选择了更加现代化的jenkins blueocean\n创建好yaml后，分别执行:\nkubectl apply -f deploy-jenkins.yaml kubectl apply -f jenkins-pv-pvc.yaml 然后执行kubectl get pods，就可以看到pods正在运行了。\nNAME READY STATUS RESTARTS AGE jenkins-695724d564-qskbk 1/1 Running 0 1s Jenkins设置 执行kubectl logs \u0026lt;your jenkins pods NAME\u0026gt;，查看日志即可看到初始密码，选择安装默认插件\n然后进入到主界面，从Manage Jenkins-\u0026gt;Manage Plugins，在available里搜索Kubernetes插件，然后安装。\n安装完后，点击Manage Nodes and Clouds\n进入后点击Clouds，然后Add new clouds，选择Kubernetes，点开detail\n首先设置k8s url, 由于jenkins是运行在k3s内，可以直接填写k8s api server的地址，可以通过在k3s里运行一个busybox来查看地址，注意busybox版本要用1.28.3，其他版本无法查看DNS\n运行一个busybox\nkubectl run -it --image=busybox:1.28.3 --restart=Never dns-test /bin/sh 查看地址\nnslookup kubernetes Server: 10.43.0.10 Address 1: 10.43.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 10.43.0.1 kubernetes.default.svc.cluster.local 将k8s url填写进去：\nk8s创建SA证书 由于jenkins连接k8s需要验证，有很多种方式可以提供访问凭证，这里仅提供SA证书的token访问方式\n首先创建一个名为jenkins的sa\nkubectl create sa jenkins 然后绑定\nkubectl create clusterrolebinding jenkins --clusterrole cluster-admin --serviceaccount=default:jenkins 然后获取该证书的一些信息\nkubectl describe sa jenkins -n default Name: jenkins Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: jenkins-token-drc5q Tokens: jenkins-token-drc5q Events: \u0026lt;none\u0026gt; 获取证书token\nkubectl describe secrets jenkins-token-drc5q -n default Name: jenkins-token-drc5q Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: jenkins kubernetes.io/service-account.uid: f2f806c9-a71d-43e9-b65c-0d3997666770 Type: kubernetes.io/service-account-token Data ==== ca.crt: 570 bytes namespace: 7 bytes token: \u0026lt;实际token内容\u0026gt; jenkins创建Credentials访问k8s 点击Add创建\nkind选择secret text，将刚刚获取到token内容填入secret，点击add\n然后Credentials选中刚刚创建的，点击旁边的test connection，如果连接成功，则显示\n可以在busybox里查看jenkins的地址\nnslookup jenkins-service Server: 10.43.0.10 Address 1: 10.43.0.10 kube-dns.kube-system.svc.cluster.local Name: jenkins-service Address 1: 10.43.82.142 jenkins-service.default.svc.cluster.local 然后填入，注意一定要填入端口号\n再往下，可以设置一下随机生成的jenkins slave pod的label\n再往下可以设置一些镜像模版，然后再jenkins流水线里直接选用，出于简洁和好管理，镜像模版这边会选择写在pipline文件里\n创建Jenkins流水线，实现CI/CD 下面是一个简单的pipeline文件\ndef label = \u0026#34;test-k8s-${UUID.randomUUID().toString()}\u0026#34; podTemplate(label: label, // See 1 containers: [ containerTemplate( name: \u0026#39;jnlp\u0026#39;, image: \u0026#39;jenkins/inbound-agent:alpine\u0026#39;, ), containerTemplate(name: \u0026#39;kubectl\u0026#39;, image: \u0026#39;yuantaosu/kubectl-alpine\u0026#39;, command: \u0026#39;cat\u0026#39;, ttyEnabled: true), containerTemplate(name: \u0026#39;docker\u0026#39;, image: \u0026#39;docker:20.10.12-alpine3.15\u0026#39;, command: \u0026#39;cat\u0026#39;, ttyEnabled: true), ], volumes: [ // See 2 hostPathVolume(mountPath: \u0026#39;/var/run/docker.sock\u0026#39;, hostPath: \u0026#39;/var/run/docker.sock\u0026#39;), // See 3 hostPathVolume(mountPath: \u0026#39;/usr/bin/docker.sock\u0026#39;, hostPath: \u0026#39;/usr/bin/docker.sock\u0026#39;), // See 3 ] ){ node(label){ container(\u0026#39;docker\u0026#39;) { stage(\u0026#39;pull code\u0026#39;) { checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/master\u0026#39;]], extensions: [], userRemoteConfigs: [[credentialsId: \u0026#39;\u0026#39;, url: \u0026#39;\u0026#39;]]]) } stage(\u0026#39;check docker\u0026#39;){ sh \u0026#39;docker build -t test-app .\u0026#39; } // stage(\u0026#39;运行 Kubectl\u0026#39;) { // withCredentials([file(credentialsId: \u0026#39;kubeconfig-file\u0026#39;, variable: \u0026#39;KUBECONFIG\u0026#39;)]) { // container(\u0026#39;kubectl\u0026#39;) { // sh \u0026#34;mkdir -p ~/.kube \u0026amp;\u0026amp; cp ${KUBECONFIG} ~/.kube/config\u0026#34; // sh \u0026#34;kubectl get pods\u0026#34; // } // } // } } } } ","date":"2022-03-05T14:29:10Z","image":"https://kinokosu3.github.io/p/k3s-jenkins-docker-%E5%8D%95%E6%9C%BA%E5%99%A8%E5%AE%9E%E9%AA%8Cci/cd/cover_hu710e61c74b4036f14c7dc8a173c95cb0_86464_120x120_fill_box_smart1_3.png","permalink":"https://kinokosu3.github.io/p/k3s-jenkins-docker-%E5%8D%95%E6%9C%BA%E5%99%A8%E5%AE%9E%E9%AA%8Cci/cd/","title":"k3s-jenkins-docker 单机器实验CI/CD"},{"content":"panic与defer的组合 在Golang的goroutine中会有一个结构体g，记录了一些关于goroutine的信息，里面有一个defer链表的头指针，还有一个panic链表的头指针，出现一个panic，就往该panic链表中塞入一个，头指针指向当前执行的那个。执行一个简单的panic：\nfunc main() { defer A1() panic(\u0026#34;main panic\u0026#34;) } main函数注册defer函数A1后，发生panic，返回去执行defer。和正常函数结束后defer不同，panic后执行defer会将defer结构体中的_panic指向当前执行的panic，用来记录这个defer是由这个panic触发的。这样做的好处是防止defer函数执行时再次发生panic，不会重复执行当前defer。\n一个关于panic-\u0026gt;defer-\u0026gt;panic的例子：\nfunc A1(){ panic(\u0026#34;A1 panic\u0026#34;) } func main() { defer A1() panic(\u0026#34;main panic\u0026#34;) } 发生panic后，首先将defer A1的started和_panic赋值，在A1也发生panic后，当前goroutine的defer链表中只有A1一个defer，而且A1已经开始且有panic指向，这时候会发生什么呢，先来看看panic结构体。panic结构体的信息如下：\ntype _panic sturct{ argp unsafe.Pointer // 在panic期间调用的defer函数 arg interface{} //panic的参数 link *_panic //链接之前的panic recovered bool //是否被恢复 aborted bool //是否被终止 } 这时候Golang会把之前的main panic的aborted置为true，表示它已经终止，然后输出panic(顺序为发生的时间正序)。\nrecover、panic、defer的组合 func A1(){ recover() } func main() { defer A1() panic(\u0026#34;main panic\u0026#34;) } main panic发生后，执行defer A1，发生recover，main panic的recovered被置为true，表示被恢复。在每个defer执行完之后，Golang都会去检查一遍这个defer的panic是否被恢复，如果恢复，则将其从panic链表中删除。\n如果带有recover的defer函数也发生panic呢？\nfunc A1(){ recover() panic(\u0026#34;A1 panic\u0026#34;) } func A2(){ } func main() { defer A2() defer A1() panic(\u0026#34;main panic\u0026#34;) } 这时候panic-\u0026gt;defer，发现defer A1已经执行，然后将panic main的aborted置为true，终止，这时候defer A2就是由A1 panic触发的了。这时候输出panic信息时，main panic出正常输出且会带有recovered字样，没有被摘除。\n","date":"2022-01-02T14:29:10Z","image":"https://kinokosu3.github.io/p/golang%E4%B8%AD%E7%9A%84panicdeferrecover/cover_hu7d1f0a795ffd4340e0d08dced279fdc1_390689_120x120_fill_box_smart1_3.png","permalink":"https://kinokosu3.github.io/p/golang%E4%B8%AD%E7%9A%84panicdeferrecover/","title":"Golang中的panic、defer、recover"},{"content":"题目： N 对情侣坐在连续排列的 2N 个座位上，想要牵到对方的手。 计算最少交换座位的次数，以便每对情侣可以并肩坐在一起。 一次交换可选择任意两人，让他们站起来交换座位。\n人和座位用 0 到 2N-1 的整数表示，情侣们按顺序编号，第一对是 (0, 1)，第二对是 (2, 3)，以此类推，最后一对是 (2N-2, 2N-1)。\n这些情侣的初始座位 row[i] 是由最初始坐在第 i 个座位上的人决定的。\n题解：\n情侣是一对对的，例如0，1为一对情侣，我们将以一对情侣进行编号，例如(0,1)的编号为0，(2,3)的编号为1，由此得出(i,i+1）的编号为i/2, 对于题目的乱序坐位，比如(34,45)，34的编号为34/2，45的编号为45/2。这样就可以将同一对情侣作为同一个连通分量来看待。 这道题关键的地方在于如何将一对情侣和打乱的情侣群看待，这里会讲一对排好的情侣看作是一个连通分量，一群打乱的情侣看作是一个连通分量，如果所有的情侣都是乱序的，那么连通分量只有1，如果有一对排好的，其他的都是乱序的，那么连通分量为2(或许？)，Union操作就是将所有一对对乱序的情侣作为同一个连通分量看待。\n知道了连通分量如何表示后，如何根据连通分量来解决问题呢？\n如果一个连通分量里有两对情侣，他们需要交换一次来完成。 如果一个连通分量里有三对情侣，他们需要交换两次来完成。\n所有最小交换次数为 情侣对数 - 连通分量数。\n代码：\nfunc minSwapsCouples(row []int) int { // 路径压缩 // 平衡优化 length := len(row) parent := make([]int,0) size := make([]int,0) count := 0 for i:=0;i\u0026lt;length;i++{ parent = append(parent, i) size = append(size,1) } Find := func (x int) int{ for{ if parent[x] == x{ break } parent[x] = parent[parent[x]] x = parent[x] } return x } Union := func (p, q int)(){ rootP := Find(p) rootQ := Find(q) if rootP == rootQ{ return } if size[rootP] \u0026gt; size[rootQ] { parent[rootQ] = rootP; size[rootP] += size[rootQ]; } else { parent[rootP] = rootQ; size[rootQ] += size[rootP]; } } for i:=0;i\u0026lt;length;i+=2{ Union(row[i] /2 ,row[i+1]/2) } for i:=0;i\u0026lt;length/2;i++{ if i == Find(i){ count++ } } return (length/2) - count } ","date":"2021-10-10T14:29:10Z","permalink":"https://kinokosu3.github.io/p/765.%E6%83%85%E4%BE%A3%E4%BA%A4%E6%8D%A2/","title":"765.情侣交换"},{"content":"题目 path-with-minimum-effort\n你准备参加一场远足活动。给你一个二维 rows x columns 的地图 heights ，其中 heights[row][col] 表示格子 (row, col) 的高度。一开始你在最左上角的格子 (0, 0) ，且你希望去最右下角的格子 (rows-1, columns-1) （注意下标从 0 开始编号）。你每次可以往 上，下，左，右 四个方向之一移动，你想要找到耗费 体力 最小的一条路径。\n一条路径耗费的 体力值 是路径上相邻格子之间 高度差绝对值 的 最大值 决定的。\n请你返回从左上角走到右下角的最小 体力消耗值 。\n输入：heights = [[1,2,2],[3,8,2],[5,3,5]] 输出：2 解释：路径 [1,3,5,3,5] 连续格子的差值绝对值最大为 2 。 这条路径比路径 [1,2,2,2,5] 更优，因为另一条路径差值最大值为 3 。 思路 很明显的最短路径问题，我一开始的思路是走单源最短路径，也就是dijkstra，准确来说是贪心。路径权值为他们的高度差。dijkstra有确定最短路径的bool数组，所以这题也能用Dp来做，保存到某点时最小高度差，状态转移为两点与该点的最小高度差。一条最短路径，当然也可以是最小生成树，将所有的边进行排序。\n这里使用最小生成树与并查集。（虽然最后写出来效率巨差草）\n思路是先对边排序，然后由于总是先会选择路径最短的边，在选择的过程中就将该点加入一个连通分量中，链接过程中比较出口和入口是否在同一个连通分量里，在则一颗最小生成树已经生成。\ntype PointLine struct{ Start int End int Cost int } type PointLineList []PointLine func(p PointLineList) Len() int { return len(p) } func(p PointLineList) Swap(i,j int){ p[i],p[j] = p[j],p[i] } func(p PointLineList) Less(i, j int) bool{ return p[i].Cost \u0026lt; p[j].Cost } func abs(n int) int { y := n \u0026gt;\u0026gt; 63 return (n ^ y) - y } func minimumEffortPath(heights [][]int) int { m := len(heights) n := len(heights[0]) minCost := 0 size := make([]int, 0) edges := make(PointLineList,0) parent := make([]int, 0) for i:=0;i\u0026lt;m;i++{ for j:=0;j\u0026lt;n;j++{ if i + 1 \u0026lt; m { edge := PointLine{ Start: i*n + j, End: (i+1)*n + j, Cost: abs(heights[i+1][j] - heights[i][j]), } edges = append(edges, edge) } if j + 1 \u0026lt; n { edge := PointLine{ Start: i*n + j, End: i*n + j + 1, Cost: abs(heights[i][j+1] - heights[i][j]), } edges = append(edges, edge) } } } for i:=0;i\u0026lt;m*n;i++{ parent = append(parent, i) size = append(size,1) } // 找根节点 Find := func (x int) int{ for{ if parent[x] == x{ break } parent[x] = parent[parent[x]] x = parent[x] } return x } Union := func (p, q int)(){ rootP := Find(p) rootQ := Find(q) if rootP == rootQ{ return } if size[rootP] \u0026gt; size[rootQ] { parent[rootQ] = rootP; size[rootP] += size[rootQ]; } else { parent[rootP] = rootQ; size[rootQ] += size[rootP]; } } isSame := func(p, q int) bool{ return Find(p) == Find(q) } sort.Sort(edges) for i:=0;i\u0026lt;edges.Len();i++{ minCost = edges[i].Cost Union(edges[i].Start, edges[i].End) if isSame(0,m*n-1) { break } } return minCost } ","date":"2021-10-06T14:29:10Z","permalink":"https://kinokosu3.github.io/p/1631.%E6%9C%80%E5%B0%8F%E4%BD%93%E5%8A%9B%E6%B6%88%E8%80%97%E8%B7%AF%E5%BE%84/","title":"1631.最小体力消耗路径"},{"content":"暑假已经过了一半了，复习的进度也大概过了一轮了（数学），对于这次课设，已经过了一段时间了，也是现在才突然想起来写一个随笔记一下过程，毕竟在我的傻屌大学里，也是少有的稍微有点规模的课程设计，前后的时间用了差不多一周，在交工前一天才主要完成了，因为我没怎么听课（哈哈哈哈），对于课题需要实现到怎么样子也是完成按照自己的想象来完成，到了答辩才发现我好像弄了一套和别人技术程度完成不一样的东西。之前也是完成没有弄过这方面的，也是边翻书边查资料来写，但是都不靠谱，弄的我走了无数的弯路，最后靠着自己的理解来完成了。还算挺有意思的。\n初步：大概实现和电路仿真 拿到课题的时候，我就想着如何实现通信级别的东西，我也完全是半桶水，在看书思考的时候也完全没有考虑什么，教科书式的写了一套多机通信的代码，一开始写了个最简单的一对二，在仿真里调试，屁颠屁颠的写了什么校验码，收发数据握手，一大堆通信特性，代码也删删改改，迭代了几个版本，完成了一个能在仿真里跑的多机通信代码模型，拿到实机一跑，就掉进了真正的坑。跑完发现全没动静，串口调试也完全是乱码，完全不行，最后发现有线通信代码是可用的，拿两根线把单片机的RX/TX反接，完全ok，最后在某网站上的一篇资料发现了一句话，这句话也代表了整个项目的坑：\n\u0026ldquo;有线通信时收发数据的时间间隔近似于0，无线通信时会有时间间隔\u0026rdquo;\n主要：无线通信代码和乱码debug 想通之后，没办法，靠教科书式，复制粘贴来的代码是没啥卵用了，自己对着通信步骤自己写一套吧，然后就开始重新写。之前因为需要仿真的缘故，软件只能在windows环境下运行，就没有使用mac，现在因为都是实机测试，就不需要Windows了，所以转到mac下完成代码。打了串口驱动，装了C51编译，找了个烧录脚本，都是mac下用的。好像到最后跑都是mac下编译出来的程序，当然展示时候用的keil格式的C代码，没有用这边编译器的版本。在写的时候也是不停de各种bug，比如收的代码放在中断，发就随便根据标示写，每次都开串口中断都要判断是否是收的数据。这个方式就解决双方握手之后时间无法对上，导致无法进行代码的情况，在收发之前都加了相近的时间挂起，主要细节不太记得了，反正好像双机ok，三机又不行了，后来一周都是不停的对时间间隔的时间进行调参。这时候很明显的一个bug，传输数据bug，运行出现死循环。都是收发时间对不上导致的。\n完成： 画出时间轴，修改了时间间隔时间，实现无乱码同步 一直调试了几天的时间，最后在交工前，突然想起时间轴的问题，对每一步就仔细对准时间，然后就完成了整套通信的乱码问题。在一路下来，从很复杂的有着很多特性通信模型，一路简化下来，就只剩了一个握手收发的框架，在最后这段时间，在修改时间间隔解决乱码的中途，也根据课题集成了传感器代码进来，对每一个任务需求进行完成。也成功完成了答辩（这不是肯定的吗）。\n后记 这次实训（算是吧）能有一个那么开放的过程也是因为老师，这次的物联网课老师好像是新来的，不太清楚我们这些傻屌学生的实力，就布置了一个较为开放性的课设。算是较为深度的训练了一次动手能力，虽然代码写了很多套都推翻了，越写越简单哈哈哈哈，大学生涯应该是没有机会了。也挺感概的。\n代码开源了，虽然老师不想让我公开的，还想让我挂去展示厅，发傻屌中文期刊，我觉得我没有什么求名求利的心情，也算不上是什么很牛逼的东西，最后开始公开了，Open Source Spirit最重要嘛。\nGitHub链接：B405-course-design\n","date":"2019-08-09T18:33:02Z","permalink":"https://kinokosu3.github.io/p/%E4%B8%80%E6%AC%A1%E7%89%A9%E8%81%94%E7%BD%91%E8%AF%BE%E8%AE%BE%E7%9A%84%E5%9B%9E%E9%A1%BE/","title":"一次物联网课设的回顾"},{"content":"基本操作 字符串 数字转字符串 std::to_string() math函数 求绝对值函数 int abs()、long labs()、double fabs() 取整函数 floor()是向负无穷大舍入，floor(-10.5) == -11、ceil()是向正无穷大舍入，ceil(-10.5) == -10 ","date":"2019-05-22T22:52:21Z","permalink":"https://kinokosu3.github.io/p/c-%E8%A7%A3%E9%A2%98%E6%8A%80%E5%B7%A7/","title":"C++解题技巧"},{"content":"回来了 妈耶，上一次更博客已经是半年前了，那还是看到大佬的博客的时候写了一篇come-back，现在打算重新整理博客却找不到12月8号版本的了，只有7月版本的，这期间完成了一场高校的数据平台比赛，写了几篇关于Hadoop ecology的文章，这些文章应该没有办法重新写了，如果以后找到时间应该会重新照着之前的格式重新写一篇，那些关于Hadoop文章也没有什么东西看的，都是环境的布置。之所以一直没有打理博客是因为一直为了考研东奔西走的(似乎大佬也在为考研做准备)，哈哈哈，越是往上走越是发现学校的重要性，到这种人生的十字路口，已经发现只有学校才是向上走的最佳途径了。\n之前在fliboard上看过一篇文章，是一个香港富豪，去体验清洁工的生活，他说的几句话让我记忆深刻，他说在这种如此辛苦的状态下，根本没有办法去努力，去突破现状，完全被生活支配。一旦去了工作，就难以挣扎出来，还是在学校能保持那种学习的欲望。去越好的地方，才能找到志同道合的人，才能不再孤独。\n","date":"2019-05-21T22:07:21Z","permalink":"https://kinokosu3.github.io/p/%E6%97%B6%E9%9A%94%E5%8D%8A%E5%B9%B4%E9%87%8D%E6%96%B0%E6%95%B4%E7%90%86%E5%8D%9A%E5%AE%A2/","title":"时隔半年重新整理博客"},{"content":"This is a late diary.\nMy English has never improved.\noh, I do not want. Maybe I’m very lazy recently.\nA couple of was Chinese Valentine’s Day. But I am in Zhuhai a few ago day.\nMy girlfriend gave me a gift. Of course, after appointment time.\nThat’s so surprised. Maybe. Because she sent picture of gift before.\nI am touching. This is my first gift in my life. No one has ever given a gift. The sound so painful. The strong was single.\nI love her, although so disgusting, she is my only female friend.\nActually, Hong Kong, super expensive food. I feel Hong Kong people all have iPhone.\n","date":"2018-08-19T23:01:40Z","permalink":"https://kinokosu3.github.io/p/%E4%B8%83%E5%A4%95%E5%90%8En%E5%A4%A9%E6%97%A5%E8%AE%B0/","title":"七夕后N天日记"},{"content":"solution 最大深度 做一个前序遍历，然后比较一下左右子树返回。\n注意:\n两个相同的数进行比较，返回0 class Solution { public: int maxDepth(TreeNode* root) { if(root == NULL) return 0; int leftDepth = maxDepth(root-\u0026gt;left); int rightDepth = maxDepth(root-\u0026gt;right); return leftDepth \u0026gt; rightDepth ? (leftDepth + 1) : (rightDepth + 1); } }; 最小深度 这题的主要思路和最大深度差不多，但是有一个最坏情况需要考虑，就是会出现斜树的情况，也就是只有左子树或者右子树的时候。\n我们采用的方式是判断左子树或右子树是否为空，若左子树为空，则返回右子树的深度，反之返回左子树的深度，如果都不为空，则返回左子树和右子树深度的最小值。\nint minDepth(TreeNode *root) { // write your code here if(root == NULL) return false; if(root-\u0026gt;left == NULL) return minDepth(root-\u0026gt;right) + 1; if(root-\u0026gt;right == NULL) return minDepth(root-\u0026gt;left) + 1; int leftDepth = minDepth(root-\u0026gt;left); int rightDepth = minDepth(root-\u0026gt;right); return leftDepth \u0026lt; rightDepth ? (leftDepth + 1) : (rightDepth + 1); } ","date":"2018-07-30T16:53:55Z","permalink":"https://kinokosu3.github.io/p/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%9C%80%E5%A4%A7%E6%B7%B1%E5%BA%A6%E5%92%8C%E6%9C%80%E5%B0%8F%E6%B7%B1%E5%BA%A6/","title":"二叉树的最大深度和最小深度"},{"content":"给定一个数组，将数组中的元素向右移动 k 个位置，其中 k 是非负数。\n实例:\n输入: [1,2,3,4,5,6,7] 和 k = 3 输出: [5,6,7,1,2,3,4] 解释: 向右旋转 1 步: [7,1,2,3,4,5,6] 向右旋转 2 步: [6,7,1,2,3,4,5] 向右旋转 3 步: [5,6,7,1,2,3,4] 使用O(n)的时间复杂度的话，可以使用数组翻转公式，(i + k) % n = 旋转后的位置。 要注意计算出来的位置和原来位置相同如何处理。\nclass Solution { public: void rotate(vector\u0026lt;int\u0026gt;\u0026amp; nums, int k) { int length = nums.size(); int start = 0; int i = 0; int cur = nums[i]; int cnt = 0; while (cnt++ \u0026lt; length) { i = (i + k) % length; int t = nums[i]; nums[i] = cur; if (i == start) { ++start; ++i; cur = nums[i]; } else { cur = t; } } } }; ","date":"2018-07-29T18:27:45Z","permalink":"https://kinokosu3.github.io/p/leetcode-%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84/","title":"LeetCode-旋转数组"},{"content":"Forward 今天发现我以前居然还发过这种说说，赶快删掉原文转blog来(｀・ω・´) 事情是去年今日，2017-07-27\nraw content 突闻噩耗 SSR整个项目的github已经清除，404 not found了 写到这里我突然停顿了一下，不知道怎么写下去 这个项目的作者是一位19岁的天才代码少女\n想到我很久以前，那时候几个玩舰娘的人，素昧平生的人，因为Gfw对VPN的干扰一天比一天严重，有位大兄弟发现了SSR,这个好东西，测试了几天，大家一起拿了钱用SSR建了机场，几乎好像是昨天的事（这算是人生的第一个创业项目啊（笑））又想到突然第一次用安卓的SSR的时候，全粉红色的风格让我惊喜莫名，好像这不是一个工具，而是手机里的日常。\n消息传来，我们几个人都蒙了，服务器上跑着的脚本，突然就孤单了起来，它再也没有第二个版本出现，再也不会被它的创造者维护了。没有替代品，以后也不知道能不能找得到\n无论是客户端，还是服务器端，都再也没有官方了\n对于这件事的起因经过结果，我想了想，人性无非就是这样，利益既得者的胜利，也有作者的社交特性在里面，跟使用的人们交流，但越是这样，就越是容易被中国人可怕的丑恶的言语摧毁。\n得到时感觉理所当然，失去时才会珍惜。\n感谢它，让我浏览世界，Google，Twitter，YouTube，NicoNico，github，DMM，stackoverflow等等，所有的一切\n从开始研究的那天开始，我们几个人有的人从高中生变成大学生，有的人从大学生变成社畜，时间就那么一瞬间\n感谢它，让我在生命最好的年华接触了最美好的东西\n感谢它，陪伴我走过那么多风雨\n感谢它，中国互联网通往乌托邦的门票\n有的时候，有些东西，眨眼即是永远\n再见，shadowsocks-rss\n再见，shadowsocks协议\nいまが最高 ","date":"2018-07-27T12:22:11Z","permalink":"https://kinokosu3.github.io/p/%E5%8E%BB%E5%B9%B4%E7%9A%84%E4%B8%80%E7%AF%87%E6%84%9F%E6%83%B3/","title":"去年的一篇感想"},{"content":"question 给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, \u0026hellip;）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。\nsolution 动态规划思路 如果一个数x可以表示为一个任意数a加上一个平方数bｘb，也就是x=a+bｘb，那么能组成这个数x最少的平方数个数，就是能组成a最少的平方数个数加上1（因为b*b已经是平方数了）。\nclass Solution { public: int numSquares(int n) { vector\u0026lt;int\u0026gt; dp(n+1,0); for(int i=0;i\u0026lt;n+1;i++) dp[i]=i; //最多都由1组成 for(int i=0;i\u0026lt;=n;i++) for(int j=1;i+j*j\u0026lt;=n;j++) { cout \u0026lt;\u0026lt; \u0026#34; i: \u0026#34; \u0026lt;\u0026lt; i ; dp[i + j * j] = min(dp[i + j * j], dp[i] + 1);//要么本身，要么加一个平方数 cout \u0026lt;\u0026lt; \u0026#34; i+j*j: \u0026#34; \u0026lt;\u0026lt;dp[i + j * j] \u0026lt;\u0026lt; endl; } return dp[n]; } }; 四平方和定理 四平方和定理 （英语：Lagrange\u0026rsquo;s four-square theorem） 说明每个正整数均可表示为4个整数的平方和。\n解题定理:\n如果一个数除以8余7的话，那么肯定是由4个完全平方数组成 首先我们将数字化简一下，由于一个数如果含有因子4，那么我们可以把4都去掉。 class Solution { public: int numSquares(int n) { while (n % 4 == 0) n /= 4; if (n % 8 == 7) return 4; for (int a = 0; a * a \u0026lt;= n; ++a) { int b = sqrt(n - a * a); if (a * a + b * b == n) { return !!a + !!b; } } return 3; } }; ","date":"2018-07-26T16:42:48Z","permalink":"https://kinokosu3.github.io/p/leetcode-279.%E5%AE%8C%E5%85%A8%E5%B9%B3%E6%96%B9%E6%95%B0/","title":"LeetCode-279.完全平方数"},{"content":"Foreword 最近的一个模拟竞赛\n860. 柠檬水找零 860. 柠檬水找零 在柠檬水摊上，每一杯柠檬水的售价为 5 美元。\n顾客排队购买你的产品，（按账单 bills 支付的顺序）一次购买一杯。\n每位顾客只买一杯柠檬水，然后向你付 5 美元、10 美元或 20 美元。你必须给每个顾客正确找零，也就是说净交易是每位顾客向你支付 5 美元。\n注意，一开始你手头没有任何零钱。\n如果你能给每位顾客正确找零，返回 true ，否则返回 false 。\nclass Solution { public: bool lemonadeChange(vector\u0026lt;int\u0026gt;\u0026amp; bills) { vector\u0026lt;int\u0026gt;::iterator it; int five = 0, ten = 0, twenty = 0; for(it=bills.begin();it!=bills.end();it++){ if(*it == 5){ five++; continue; }else if(*it == 10){ if(five == 0) return false; five--; ten++; continue; }else if(*it == 20){ if(ten \u0026gt; 0){ five--; ten--; if (five \u0026lt; 0) return false; }else {five -=3;if(five \u0026lt;0) return false;} } } return true; } }; 863. 二叉树中所有距离为 K 的结点 863. 二叉树中所有距离为 K 的结点 给定一个二叉树（具有根结点 root）， 一个目标结点 target ，和一个整数值 K 。\n返回到目标结点 target 距离为 K 的所有结点的值的列表。 答案可以以任何顺序返回。\n输入：root = [3,5,1,6,2,0,8,null,null,7,4], target = 5, K = 2\n输出：[7,4,1]\n这题的难点在如何遍历到目标节点上面的符合距离的点，所以我们使用map来映射二叉树内的每一个点，queue来处理每个点的父节点、left极点、right节点，set来保存访问过的节点。\n首先遍历树，建立映射:\nvoid dfs(TreeNode *root, map\u0026lt;int, vector\u0026lt;int\u0026gt; \u0026gt; \u0026amp;a) { if (root == NULL) { return; } if (root-\u0026gt;left != NULL) { a[root-\u0026gt;val].push_back(root-\u0026gt;left-\u0026gt;val); a[root-\u0026gt;left-\u0026gt;val].push_back(root-\u0026gt;val); dfs(root-\u0026gt;left, a); } if (root-\u0026gt;right != NULL) { a[root-\u0026gt;val].push_back(root-\u0026gt;right-\u0026gt;val); a[root-\u0026gt;right-\u0026gt;val].push_back(root-\u0026gt;val); dfs(root-\u0026gt;right, a); } } 主要解决方法是将树内的全部相同距离的点聚合在一起：\nvector\u0026lt;int\u0026gt; distanceK(TreeNode* root, TreeNode* target, int K) { map\u0026lt;int, vector\u0026lt;int\u0026gt; \u0026gt; a; dfs(root, a); //将每一个值前后对应 queue\u0026lt;pair\u0026lt;int, int\u0026gt; \u0026gt; que; que.push(make_pair(target-\u0026gt;val, 0)); //合并两个值 map\u0026lt;int, vector\u0026lt;int\u0026gt; \u0026gt; res; set\u0026lt;int\u0026gt; st; st.insert(target-\u0026gt;val); while (!que.empty()) { // 判断队列是否空 auto now = que.front(); // auto是自动类型判断 res[now.second].push_back(now.first); que.pop(); for (int i = 0; i \u0026lt; a[now.first].size(); i++) { // 里面有父节点和left和right int nxt = a[now.first][i]; //下一个值，left或者right或者父节点 if (st.find(nxt) == st.end()) { //判断nxt是否在set中 st.insert(nxt); que.push(make_pair(nxt, now.second + 1)); } } } return res[K]; } 861. 翻转矩阵后的得分 861. 翻转矩阵后的得分 有一个二维矩阵 A 其中每个元素的值为 0 或 1 。\n移动是指选择任一行或列，并转换该行或列中的每一个值：将所有 0 都更改为 1，将所有 1 都更改为 0。\n在做出任意次数的移动后，将该矩阵的每一行都按照二进制数来解释，矩阵的得分就是这些数字的总和。\n返回尽可能高的分数。\n示例:\n输入：[[0,0,1,1],[1,0,1,0],[1,1,0,0]] 输出：39 解释： 转换为 [[1,1,1,1],[1,0,0,1],[1,1,1,1]] 0b1111 + 0b1001 + 0b1111 = 15 + 9 + 15 = 39 这题的解法主要是先确定首位为1，因为需要二进制最大，首位的值权重很高，再进行其余位的转换，因为不是1就是0，我们每列可以做一个最大值确定，看1的数量多还是0的数量多，哪个多就使用哪个，因为可以无限转换，计算二进制的方法也非常巧妙，用每列的1值数量乘与一个与位数减1的二进制数\nclass Solution { public: int matrixScore(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; A) { int n = A.size(); //行 3 int m = A[0].size(); //列 4 for(int i=0;i\u0026lt;n;i++){ if(A[i][0] == 0){ for(int j=0;j\u0026lt;m;j++) A[i][j] = 1 - A[i][j]; } } int ans = 0; for(int j=0;j\u0026lt;m;j++){ int p = m - j - 1; int ocnt = 0, zcnt = 0; for(int i=0;i\u0026lt;n;i++){ if(A[i][j] == 1) ocnt++; else zcnt++; } ans += max(ocnt, zcnt) * (1 \u0026lt;\u0026lt; p); } return ans; } }; ","date":"2018-07-25T17:09:12Z","permalink":"https://kinokosu3.github.io/p/leetcode-weekly-contest-91-solution/","title":"LeetCode Weekly Contest 91 solution"},{"content":"tensorflow里的loss function cross_entropy交叉熵 交叉熵刻画的是两个概率分布之间的距离，我们通过softmax回归将神经网络前向传播得到的结果变成交叉熵要求的概率分布得分。\nsoftmax函数用于多分类神经网络输出, 使得神经网络输出层输出的是一个概率分布，哪个点的概率高代表着某个类别的概率高。 sigmoid函数也用于隐层输出，使用哪个函数看业务需求\nsigmoid_cross_entropy_with_logits 这个函数的输入是logits和targets，logits就是神经网络模型中的 W * X矩阵，注意不需要经过sigmoid，而targets的shape和logits相同，就是正确的label值，例如这个模型一次要判断100张图是否包含10种动物，这两个输入的shape都是[100, 10]。注释中还提到这10个分类之间是独立的、不要求是互斥，这种问题我们成为多目标，例如判断图片中是否包含10种动物，label值可以包含多个1或0个1，还有一种问题是多分类问题，例如我们对年龄特征分为5段，只允许5个值有且只有1个值为1，这种问题可以直接用这个函数吗？答案是不可以。\n\u0026ndash; 引用TensorFlow四种Cross Entropy算法实现和应用\n对于多分类问题, 我们可以使用softmax函数。\nsoftmax_cross_entropy_with_logits 这个函数只适合单目标的二分类或者多分类问题。\n对于多分类问题，例如我们的年龄分为5类，并且人工编码为0、1、2、3、4，因为输出值是5维的特征，因此我们需要人工做onehot encoding分别编码为00001、00010、00100、01000、10000，才可以作为这个函数的输入。 也就是说，原数据集的label需要做one-hot处理才可以使用这个函数。\n具体的执行流程大概分为两步，第一步首先是对网络最后一层的输出做一个softmax。\n这里需要注意的是，这个函数返回值不是一个数，而是一个向量，如果要求交叉熵，我们要在做一步tf.resuce_sum操作，就是对向量里面的所有元素求和, 最后就能得到Hy′(y),如果要求loss，则需要做一步tf.reduce_mean操作，对向量求均值.\nwarning：\nTenosrflow中集成的交叉熵操作是施加在未经过Softmax处理的logits上, 这个操作的输入logits是未经缩放的, 该操作内部会对logits使用Softmax操作。 tf.nn.sparse_softmax_cross_entropy_with_logits softmax_cross_entropy_with_logits的易用版本`\n该函数与tf.nn.softmax_cross_entropy_with_logits()函数十分相似，唯一的区别在于labels的shape，该函数的labels要求是排他性的即只有一个正确的类别，如果labels的每一行不需要进行one_hot表示，可以使用tf.nn.sparse_softmax_cross_entropy_with_logits()。\n","date":"2018-06-07T19:09:00Z","permalink":"https://kinokosu3.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95/","title":"机器学习的一些评价方法"},{"content":"DataFrame的一些技巧操作 转置 方法加T .T\n数据合并 主要方法 pd.concat, 参数有\naxis 合并方向 axis=0 纵向合并 ignore_index 重置index ignore_index=True join 合并方式 join='outer'为预设值，因此未设定任何参数时，函数默认join=\u0026lsquo;outer\u0026rsquo;。此方式是依照column来做纵向合并，有相同的column上下合并在一起，其他独自的column个自成列，原本没有值的位置皆以NaN填充。 遍历dataframe行 方法df.iterrows(), 迭代，使用for index, row in df.iterrows(), index返回index， row返回行 应该会有相同的方法遍历的列。\n重置索引 可以使用reindex、或者重新对pandas.DataFrame.index进行赋值,如果仅仅是计数索引(比如从0开始)，可以使用语句df1.reset_index(drop=True, inplace=True)，drop是是否保存原索引列 ，inplace是否直接对dataframe改写。\n","date":"2018-06-06T16:38:26Z","permalink":"https://kinokosu3.github.io/p/pandas%E4%B8%80%E4%BA%9B%E6%8A%80%E5%B7%A7/","title":"pandas一些技巧"},{"content":"前言 最近学了Google的深度学习库，TensorFlow，这个库是真的强大，当然还有更高的封装，keras之类的，但是既然一直用着TensorFlow，就想去找个好玩的东西试试，然后看到lintcode上有一个猫狗的分类比赛，就去试试吧。做了一个周末加几天，大概做出来了，因为做深度学习需要大量的计算资源，电脑是真的跑不起来(CPU版TensorFlow),这里是我的代码实现VGG16-tranfer,所以我用了Google的免费GPU资源进行计算Colaboratory，Colaboratory 是一个 Google 研究项目，旨在帮助传播机器学习培训和研究成果。它是一个 Jupyter 笔记本环境，不需要进行任何设置就可以使用，并且完全在云端运行。然后数据集和模型通过和Google Drive进行数据交互得到，别的不说，确实用GPU算的很快，无论是训练的时候还是验证的时候。\nColaboratory分配的计算资源 {%asset_img Colaboratory.png Colaboratory%}\n据说可以免费的使用Tesla k80 GPU加速,除了一些简单的debug在本地测试一下,我都想全部丢在上面跑了。。。\n初步思路 之前只做过mnist数据集的训练(神经网络级别的hello world),mnist数据集的宽高是28x28x1,1代表了这个图是一通道的，所以计算量很小，我们要用什么网络实现呢，我一开始选的LetNet,并且重头开始训练(现在看来真是天真的不行)\nLetNet网络图示 {%asset_img 1.png let-net%}\n因为数据集是一个有20000张带label的图片,一旦输入数据处理的不对,对内存和显存都有着很大的压力,直接使用PIL把图像转成矩阵或者使用OpenCV(但是我觉得前期处理可以)转化应该是不行的了，然后接触到了TFRecord这种TensorFlow特有的数据集格式,我会另外写一篇来介绍Tfrecord,第一次转化的时候就被坑死了。。\n一开始考虑到训练的难度,没有打算直接输入3通道图片进去训练,因为有着之前一个写好的一个LetNet,在单矩阵上做卷积,一开始的图片处理打算是全部压缩成灰度图,然后输入CNN里面进行训练,之后进行训练的时候(本机CPU训练),差点弄死我了，内存算上压缩的高达60GB+(Liunx会有swap区),我已经听到我的SSD在尖叫了。。。\nTensorFlow压缩灰度图操作\n# img_data读取的图片数据 # plt是import matplotlib.pyplot as plt image_data = tf.image.rgb_to_grayscale(img_data) plt.imshow(image_data.eval()[:, :, 0], cmap=\u0026#39;gray\u0026#39;) plt.show() 初次实验改良搭的图,TensorBoard显示 {%asset_img LetNet.png LetNet%}\n在图上就可以看出,FC层非常硕大，这是因为全连接的神经网络参数众多，训练起来非常庞大，卷积神经网络CNN诞生的其中一个目标就是缩减参数，但是在一开始的时候我只搭了大概3层卷积层(后来使用VGG16甚至有着13层卷积层)，然后在FC层诞生了一个几十万参数的矩阵进行乘法的操作。。。然后内存就爆了(那时候还在用CPU)，通过性能可视化可以看到内存变化，tf.RunMetadata()，然后又加了两层卷积层，然后训练还是很难，然后就打算重构了。\n这个代码可以在Github的LetNet_error找到\nTfrecord首坑 在打包train数据集的时候，写错了几行代码，导致跑了一个晚自习，后来发现一台卡死的电脑和一个高达80G的tfrecord文件 (´-﹏-`；)\n本来正常操作应该是这样的(伪代码)\ndef _bytes_feature(value): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])) for name in names: image_raw_data = tf.gfile.FastGFile(\u0026#39;../train/\u0026#39;+name, \u0026#39;rb\u0026#39;).read() example = tf.train.Example(features=tf.train.Features(feature={ \u0026#39;image\u0026#39;: _bytes_feature(image_raw_data),})) 但是我读取之后使用了jpeg解码。。。把解码后的数据读进去了，然后几何倍数般的文件大小增长。\nimg_data = tf.image.decode_jpeg(image_raw_data) img_data = tf.image.convert_image_dtype(img_data, dtype=tf.float32) VGG16 迁移学习 最后一直在思考如何使用高正确率的网络(之前的网络能跑是能跑，但是正确率有点低，训练集都这样了，拿到测试集不完蛋了)，后来想到了之前的CTPN提取feature map时候使用的是VGG16，然后就去了解了VGG16，顺便了解了迁移学习，经过一段时间的了解，明白了迁移学习在TensorFlow里的主要实现机制，在TensorFlow里进行梯度下降之类的优化操作(反向传播BP)是使用优化器(optimizer)，而这个优化器优化的都是TensorFlow里的变量tf.Variable，我们把需要优化的参数使用变量，固定的参数使用常量之类的就好了，这样就很简单了。\n经过很多层卷积层和池化层之后，参数到FC分类层的时候确实变得很少了，而且迁移学习仅仅是训练后面几个层而已，一开始是仅仅训练最后的FC层，在数据集上获得了大概70的正确率。\n大概图(VGG主体)(tensorboard):\n{%asset_img VGG16.png VGG16%}\n理论图:\n{%asset_img VGG16-map.png VGG16%}\n之后为了提高正确率，采用了数据提升的手段，这样我们的模型将看不到任何两张完全相同的图片，这有利于我们抑制过拟合，使得模型的泛化能力更好。(来自keras文档),在这里我使用了图片翻转、调整图片色彩(亮度、对比度之类的)。\n# 随机调整图像的色彩 def distort_color(image, color_ordering=0): if color_ordering == 0: image = tf.image.random_brightness(image, max_delta=32. / 255.) image = tf.image.random_saturation(image, lower=0.5, upper=1.5) image = tf.image.random_hue(image, max_delta=0.2) image = tf.image.random_contrast(image, lower=0.5, upper=1.5) else: image = tf.image.random_saturation(image, lower=0.5, upper=1.5) image = tf.image.random_brightness(image, max_delta=32. / 255.) image = tf.image.random_contrast(image, lower=0.5, upper=1.5) image = tf.image.random_hue(image, max_delta=0.2) return tf.clip_by_value(image, 0.0, 1.0) image = tf.image.random_flip_left_right(image) image = distort_color(image, np.random.randint(2)) L2正则化、dropout这些实现就不赘述了。\n验证test 我们训练网络之后，选择一个正确率较高的模型固化成ckpt格式,然后在转化成pb格式的模型以供使用，pb模型提供输入就可以得到输出了，比ckpt好用，在验证的时候也有诸多问题，比如机器配置不够。。。到最后测试集都是分开两次进行验证的，一次性读入进行验证对内存显存的占用的太可怕了，到了最后也只能转为TFrecord文件来读取，而且还分了两次，因为一次性无法全部验证，就在遍历TFrecord文件上跳了坑。。。\nTFrecord验证集神坑 在使用测试集的tfrecord的时候，读取数据使用了队列+tf.train.batch，就跟训练的时候一样。我想如果我一次batch读取50个，遍历一次应该用100次就能遍历完了(测试集5000张图)，但是。。这个读取方法会有重复数据读取，而不会抛出tf.errors.OutOfRangeError错误，在后来使用pandas对结果排序的时候，发现在100次里面是随机读取的数据，读取的数据重复了3次，只能去找不重复的遍历方法，后来在一篇文章发现了将TFrecord的数据转成迭代器使用的方法数据集的使用方法，主要实现：\ndef parse(record): features = tf.parse_single_example( record, features={ \u0026#39;image\u0026#39;: tf.FixedLenFeature([], tf.string), \u0026#39;label\u0026#39;: tf.FixedLenFeature([], tf.int64), \u0026#39;height\u0026#39;: tf.FixedLenFeature([], tf.int64), \u0026#39;width\u0026#39;: tf.FixedLenFeature([], tf.int64), \u0026#39;channels\u0026#39;: tf.FixedLenFeature([], tf.int64), } ) image, label = features[\u0026#39;image\u0026#39;], features[\u0026#39;label\u0026#39;] height = tf.cast(features[\u0026#39;height\u0026#39;], tf.int32) width = tf.cast(features[\u0026#39;width\u0026#39;], tf.int32) # 图片解码 decoded_image = tf.image.decode_jpeg(image) # 图片转换类型 decoded_image = tf.image.convert_image_dtype(decoded_image, dtype=tf.float32) image = tf.reshape(decoded_image, [height, width, 3]) image = tf.image.resize_images(image, [224, 224], method=np.random.randint(0, 3)) return image, label dataset = tf.data.TFRecordDataset(file) datatset = dataset.map(parse) # parse函数主要是来处理数据集 test_dataset = datatset.batch(batch_size=50) test_iterator = test_dataset.make_initializable_iterator() # 读取数据，可用于进一步计算 test_image_batch, test_label_batch = test_iterator.get_next() with tf.Session() as sess: sess.run(test_iterator.initializer) image_batch, label_batch = sess.run([test_image_batch, test_label_batch]) 这样就可以遍历整个TFrecord了，这个东西真的坑死我了。\n后记 这次比赛是真的认识了很多东西，主要都是TensorFlow的实现，然后就是完全熟练的使用Colaboratory，这个东西确实对于没有计算资源的学习者有很大的帮助啊。深深认识到没有计算资源基本玩不了深度学习。。。。\n","date":"2018-05-16T17:18:28Z","permalink":"https://kinokosu3.github.io/p/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%8C%AB%E7%8B%97%E5%88%86%E7%B1%BB%E7%BA%BF%E4%B8%8A%E6%AF%94%E8%B5%9B/","title":"记一次猫狗分类线上比赛"},{"content":"前言 TensorFlow实现\n多层网络(使用多层的权重) 以下是三个隐含层的全连接方式的神经网络\nimport tensorflow as tf l1 = tf.matmul(x, w1) l2 = tf.matmul(l1, w2) y = tf.matmul(l2,w3) 激活层(引入激活函数，让每一层去线性化) 激活函数有多种，例如常用的： tf.nn.relu tf.nn.tanh tf.nn.sigmoid tf.nn.elu\nimport tensorflow as tf a = tf.nn.relu(tf.matmul(x, w1) + biase1) y = tf.nn.relu(tf.matmul(a, w2) + biase2) 损失函数 在分类问题中,交叉熵（cross entropy） 用于计算预测结果矩阵Y和实际结果矩阵Y_之间的距离\nimport tensorflow as tf cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0))) 一般我们都会把交叉熵和softmax回归一起使用\nimport tensorflow as tf cross_entropy = tf.nn.softmax_cross_entropy_with_logits(y, y_) 如果是回归问题，我们使用均方误差\nimport tensorflow as tf mse_loss = tf.reduce_mean(tf.square(y_ - y)) # 与以下函数计算结果完全一致 dataset_size = 1000 mse_loss = tf.reduce_sum(tf.pow(y_ - y, 2)) / dataset_size 训练优化器 一般优化器的目标是优化权重W和偏差biases，最小化损失函数的结果 以下优化器会不断优化W和biases\nimport tensorflow as tf LEARNING_RATE = 0.001 mse_loss = tf.reduce_mean(tf.square(y_ - y)) train_op = tf.train.AdamOptimizer(LEARNING_RATE).minimize(mse_loss) 优化学习率 学习率设置过大可能导致无法收敛，学习率设置过小可能导致收敛过慢\nimport tensorflow as tf global_step = tf.Variable(0) learning_rate = tf.train.exponential_decay( learning_rate=0.1, global_step=global_step, decay_steps=100, decay_rate=0.96, staircase=True, name=None ) train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step) 过拟合问题(正则化) 避免训练出来的模型过分复杂，即模型记住了所有数据（包括噪声引起的误差）因此需要引入正则化函数叠加的方式，避免模型出现过拟合\nimport tensorflow as tf v_lambda = 0.001 w = tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1)) y = tf.matmul(x, w) mse_loss = tf.reduce_mean(tf.square(y_ - y) + tf.contrib.layers.l2_regularizer(v_lambda)(w)) 滑动平均模型 用于控制模型的变化速度，可以控制权重W以及偏差biases 例如：avg_class.average(w) avg_class.average(biases)\nimport tensorflow as tf v1 = tf.Variable(0, dtype=tf.float32) step = tf.Variable(0, trainable=False) ema = tf.train.ExponentialMovingAverage(decay=0.99, num_updates=step) # 每一次操作的时候，列表变量[v1]都会被更新 maintain_averages_op = ema.apply([v1]) with tf.Session() as sess: # 初始化 init_op = tf.global_variables_initializer() sess.run(init_op) print(sess.run([v1, ema.average(v1)])) # 更新step和v1的取值 sess.run(tf.assign(step, 10000)) sess.run(tf.assign(v1, 10)) sess.run(maintain_averages_op) print(sess.run([v1, ema.average(v1)])) ","date":"2018-05-02T15:48:50Z","permalink":"https://kinokosu3.github.io/p/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%BB%E8%A6%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/","title":"神经网络主要优化方法"},{"content":"基础概念 计算图 tensorflow上的每一个计算都是计算图上的一个节点，节点之间的边描述了计算之间的依赖关系，TensorFlow会默认维护一个计算图。在tensorboard中可以看到可视化的计算图。\n张量 定义一个张量:a = tf.constant([1.0, 2.0], name='a')，name是这个张量的名字，张量几乎就是多维数组，张量有3个属性，名字(name)、维度(shape)、类型(type)。张量本身不存储数字。\n常量 tf.constant() 变量 tf.Variable() 占据位置的常量(好像是) tf.placeholder()\n使用时候sess.run(feed_dict={}),一个变量一个键值\n会话 使用上下文来管理会话的关闭和开启，with tf.Session() as sess: sess.run()\nTensorFlow方法 张量方法 .get_shape 获取维度信息\n数学方法 tf.matmul(x, y) x,y矩阵相乘\n随机数生成方法 tf.random_normal 正太分布 tf.truncated_normal 正态分布，随机出来的值偏离平均值超过2个标准差，这个数会随机 tf.random_uniform 平均分布 tf.random_gamma Gamma分布\n常数生成方法 tf.zeros 产生0的数组 tf.zeros([2,3]) 产生一个[[0,0,0], [0,0,0]] tf.ones 产生1的数组 tf.fill 产生一个全部为给定数字的数组 tf.fill([2,3], 9) tf.constant 产生一个常量\n初始化所有变量 init_op = tf.global_variables_initializer() sess.run(init_op)\n","date":"2018-04-23T22:31:32Z","permalink":"https://kinokosu3.github.io/p/tensorflow%E5%9F%BA%E7%A1%80%E5%A4%87%E5%BF%98%E5%BD%95/","title":"tensorflow基础备忘录"},{"content":"开启了二级验证的GitHub的push验证，使用github access token登录\n","date":"2018-04-14T10:37:02Z","permalink":"https://kinokosu3.github.io/p/github%E4%BA%8C%E7%BA%A7%E9%AA%8C%E8%AF%81%E7%99%BB%E5%BD%95/","title":"github二级验证登录"},{"content":"前言 给定样本$\\vec x$,我们用列向量表示该样本$\\vec x$=$(x1,x2,x3)^T$。样本有n种特征，我们用x(i)表示样本$\\vec x$的第i个特征。\n线性模型(linear model)的形式为:\nf($\\vec x$) = $\\vec w$·$\\vec x$ + b\n$\\vec w$是每个特征对的权重生成的权重向量\n代价函数 我们来了解一个机器学习里非常重要的概念，代价函数(cost function)比如我们有一个模型y = $\\theta 0$+$\\theta 1$x,我们要确定$\\theta 0$和$\\theta 1$的值\n我们简单的定义几个值:\n{%asset_img 1.png linear_cost_function_1%}\n这是一个简单的数据集，我们要使我们的直线尽可能的靠近这些点(拟合):\n{%asset_img 2.png linear_cost_function_2%}\n我们如何评定？拿f(x) 和y比较，我们要使这两个的值的差异为最小，这个就是一个误差最小化问题，\n我们拿每一个误差相加平方，得公式(求误差均值):$\\frac{1}{2m}\\sum_{i=1}^m(f(x)^i - y^i)^2$\ncost function和模型的对比:\n{% asset_img 3.png cost_function %}\n2主要用来化掉该式偏导后得出的常数\n上面仅仅是一个特征的代价函数，下面是两个特征的代价函数:\n{% asset_img 4.png double_cost_function %}\n代价函数求最优解 我们如何求到最小值，现在来讲解两种方法，最小二乘法和梯度下降法,\n我们回忆一下导数的意义:\n导数反映的是函数y=f(x)在某一点处沿x轴正方向的变化率。再强调一遍，是函数f(x)在x轴上某一点处沿着x轴正方向的变化率/变化趋势。直观地看，也就是在x轴上某一点处，如果f’(x)\u0026gt;0，说明f(x)的函数值在x点沿x轴正方向是趋于增加的；如果f’(x)\u0026lt;0，说明f(x)的函数值在x点沿x轴正方向是趋于减少的。\n其实就是函数中某点的切线斜率。\n偏导数也是一样，不过是对于多元函数,偏导数表示固定面上一点的切线斜率。比如我们发现ƒ在某点与xOz平面平行的切线的斜率是3，记为$\\frac{\\partial f}{\\partial x}$ = 3\n最小二乘法:\n我们对式内的特征进行偏导，然后对偏导后的式子等于0，就可以求出每个特征权重最优解的闭式解。 这是直接求出最优解的一个方法，我们重点讲一下梯度下降(Gradient Descent):\n梯度下降(Gradient Descent):\n公式:\n{% asset_img 5.png Gradient_Descent_1%}\n我们反复执行这个公式，直到收敛，这个公式怎么来的呢？？$\\alpha$是学习速率，就是我们每一次收敛时候所走的步数。\n一个收敛的例子:\n{% asset_img 6.png Gradient_Descent%}\n","date":"2018-04-09T22:15:08Z","permalink":"https://kinokosu3.github.io/p/%E5%88%9D%E8%AF%86%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E8%AE%B2%E4%B9%A0%E7%94%A8/","title":"初识机器学习-线性模型(讲习用)"},{"content":"前言 现在前端框架的乱战从三强分立(React,Vue,AngularJS)逐渐变成了两强(React,Vue),虽然一直在做着数据分析的工作，但是还是想把一些东西直接放在web上可视化或者交互，趁着有一个清明假期就去学习了一下Vue框架(React接触过之后感觉有些难懂)，以下是一些备忘录,细节不会说的太多，主要实现细节可以baidu or Google\n现代前端较为复杂，有了webpack之后，更像是软件开发了(模块化带来的错觉)\nES6/ES5+了解一下\n使用的版本：\nVue 2.9.3 npm 5.6.0 Vue-router 3.0.1 webpack 3.11.0 命令行的操作使用windows10下的console emulator(所以所有的命令都通配Liunx，但是要注意路径) npm国内安装会稍微慢，如果嫌慢可以科学上网\nWebpack了解 webpack中文文档\nscss了解 scss中文网\nVue-cli的基础使用 首先，对web开发三巨头(HTML,CSS,JavaScript)还不熟练的，可以单网页练习使用Vue,但是我们可以使用官方推荐的脚手架，脚手架使用webpack可以直接打包，官方帮助我们设定了webpack的打包文件，免去我们编写webpack配置的痛苦，我们可以专心开发Vue的组件。\n对于npm的本地和全局安装，反正我喜欢本地安装(python virtualenv的后遗症)，我们安装Vue-Cli：\nnpm install vue-cli --save-dev 我们初始化一个webpack模板:\n./node_modules/.bin/vue init webpack my-project my-project是我们项目的名字\n接下来会有一大堆设置的东西，如果看不懂直接一路回车跳过\n进入我们的项目\ncd my-project 项目目录:{%asset_img 1.png dir%}\n然后跑一下，启动命令npm start\nnpm 命令的自定义可以在package.json里面的scripts键值看到，默认的命令重写是: \u0026ldquo;scripts\u0026rdquo;: { \u0026ldquo;dev\u0026rdquo;: \u0026ldquo;webpack-dev-server \u0026ndash;inline \u0026ndash;progress \u0026ndash;config build/webpack.dev.conf.js\u0026rdquo;, \u0026ldquo;start\u0026rdquo;: \u0026ldquo;npm run dev\u0026rdquo;, \u0026ldquo;unit\u0026rdquo;: \u0026ldquo;jest \u0026ndash;config test/unit/jest.conf.js \u0026ndash;coverage\u0026rdquo;, \u0026ldquo;e2e\u0026rdquo;: \u0026ldquo;node test/e2e/runner.js\u0026rdquo;, \u0026ldquo;test\u0026rdquo;: \u0026ldquo;npm run unit \u0026amp;\u0026amp; npm run e2e\u0026rdquo;, \u0026ldquo;build\u0026rdquo;: \u0026ldquo;node build/build.js\u0026rdquo; }, 里面我们可以很OK的看到，其实npm start是等价于npm run dev的。\n我们启动的这个命令是热重载，是webpack的一个非常牛逼的功能，他会启动一个简单的server，你每次改代码它都会实时显示出来。主要细节可以baidu or Google\n命令启动完成后，我们得到一个本地服务器地址，访问它:\n{% asset_img 2.png server_url%}\n得到hello world界面:\n{% asset_img 3.png hello world %}\n简单的vue-cli就搭建好了\n","date":"2018-04-09T10:47:39Z","permalink":"https://kinokosu3.github.io/p/vue%E4%BB%8E%E9%9B%B6%E5%85%BB%E6%88%90/","title":"Vue从零养成"},{"content":"前言 中文的opencv-python内容太少了！！！！没办法只好啃英文文档了,官方文档saikou！！！\n地址OpenCV-Python Tutorials\n入门 基本操作 cv2.imread（），cv2.imshow（），cv2.imwrite（）\ncv2.imread(),可以使用以下3个flag:\ncv2.IMREAD_COLOR：加载彩色图像。图像的任何透明度都将被忽略。这是默认标志。 cv2.IMREAD_GRAYSCALE：以灰度模式加载图像 cv2.IMREAD_UNCHANGED：加载包含Alpha通道的图像 代号是1,0,-1\nimg = cv2.imread('1.jpg', 0)\ncv2.imshow()\ncv2.waitKey（）是一个键盘绑定函数，如果传递0，他将无限期等待键盘输入，敲击后，程序会继续，也可以使用特定的按键。\ncv2.destroyAllWindows（）只是销毁我们创建的所有窗口。\n如果你想销毁任何特定的窗口，使用函数cv2.destroyWindow（）作为参数传递确切的窗口名称.\n如果你先创建了窗口，再创建图片，可以使用cv2.namedWindow(),有两个flag,默认cv2.WINDOW_AUTOSIZE,cv2.WINDOW_NORMAL可以调整窗口大小。\ncv2.imwrite() 保存一张图片 cv2.imwrite('messigray.png',img)\nMatplotlib使用 一个简单的例子:\nimport numpy as np import cv2 from matplotlib import pyplot as plt img = cv2.imread(\u0026#39;messi5.jpg\u0026#39;,0) plt.imshow(img, cmap = \u0026#39;gray\u0026#39;, interpolation = \u0026#39;bicubic\u0026#39;) plt.xticks([]), plt.yticks([]) # to hide tick values on X and Y axis plt.show() matplotlib.pylot.imshow选项:\ncmap colormap-example\nperceptual uniform sequential colormaps：感知均匀的序列化 colormap sequential colormaps：序列化（连续化）色图 colormap； gray：0-255 级灰度，0：黑色，1：白色，黑底白字； gray_r：翻转 gray 的显示，如果 gray 将图像显示为黑底白字，gray_r 会将其显示为白底黑字； diverging colormaps：两端发散的色图 colormaps； seismic qualitative colormaps：量化（离散化）色图； miscellaneous colormaps：其他色图； 其实说白了就是一个色彩参照\ninterpolation 内插法选择\n有很多选择，比如:\nnearest 最近邻内插 bilinear 双线性内插 bicubic 双三次内插 视频使用 暂不了解\n绘制函数 开始之前我们先讲一个问题\n图像通道问题 较难理解，知乎有一个同理问题如何通俗易懂地讲解 Photoshop 中的「通道」概念？这个通俗易懂，这个就是彩色图像模型，下面的解释来自OpenCV学习笔记（7）图像的通道（channels）问题\n图像的通道只有1,3,4通道，因为我们描述一个像素点，如果是灰度，我们一个通道就可以了，如果一个像素点，有RGB三种颜色来描述它，就是三通道.4通道大概是加一个A通道(RGBA)，透明度，又叫alpha通道.\n线段 传递起始和结束坐标、颜色、厚度 比如蓝色的厚度5px的线段 cv2.line(img,(0,0),(511,511),(255,0,0),5)\n矩形 左上角和右下角的坐标 cv2.rectangle(img,(384,0),(510,128),(0,255,0),3)\n圆 中心坐标和半径 cv2.circle(img,(447,63), 63, (0,0,255), -1)\n多边形 顶点坐标，需要int32类型，将其转换成rowsx1x2的形状的数组，rows为顶点数\npts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32) # newshape重置参数 行列每个 pts = pts.reshape((-1,1,2)) cv2.polylines(img,[pts],True,(0,255,255)) cv2.polylines用于绘制多组线，line只能绘制一组\n","date":"2018-03-22T21:38:48Z","permalink":"https://kinokosu3.github.io/p/opencv-python%E5%85%A8%E5%85%BB%E6%88%90/","title":"OpenCV-python全养成"},{"content":"一幅图像可以定义成一个二维函数,其中x和y是空间坐标(平面),而在任何一对空间坐标(x,y)处的幅值f成为图像在该点处的强度或者灰度,当x,y和灰度值f是有限的离散数值时, 我们成为该图像为数字图像。\n灰度是表明图像明暗的数值，即黑白图像中点的颜色深度，范围一般从0到255，白色为255 ，黑色为0，故黑白图片也称灰度图像。灰度值指的是单个像素点的亮度。灰度值越大表示越亮。\n图像形成模型 0 \u0026lt; f(x, y) \u0026lt; ∞ 函数f(x, y)可以由两个分量来表示:入射分量和反射分量,分别表示为i(x,y),r(x,y),f(x, y) = i(x,y)r(x,y)\n0 \u0026lt; i(x, y) \u0026lt; ∞\n0 \u0026lt; r(x, y) \u0026lt; 1\n单色图像在任何坐标(x0, y0)处的强度(灰度)表示为:\nι = f(x0, y0) 由上式得取值范围:\nLmin ≤ ι ≤ Lmax 区间[Lmin, Lmax]成为灰度级(强度级), 实际情况常常令该区域为[0,L-1],其中ι=0为黑色,ι=L-1在灰度级中为白色。所有中间值是从黑色到白色之间变化的灰度色调。\npython-opencv示例:\nIN[1]: import numpy as np import cv2 img = cv2.imread(\u0026#39;1.jpg\u0026#39;, 0) img.min(), img.max() OUT[1]:(27, 255) 图像内插 从根本上看，内插是用已知数据来估计未知位置的数值的处理。比如调整图像的大小(收缩和放大)，这是基本的图像重取样方法。\n最近邻内插法 比如一幅图由500x500放大到750x750，先创建一个750x750的网格，然后缩小和原图像匹配，显然收缩后的750x750网格像素间隔要小于原图像的像素间隔。然后我们在原图像中寻找最接近的像素，并把该像素的灰度赋给750x750网格的新像素。当我们完成对网格中所有点的灰度赋值后，就把图像扩展到原来规定的大小，得到图像。\n这种方法把原图像中最近邻的灰度赋给了每个新位置。\n双线性内插 我们使用4个最近邻去估计给定位置的灰度，由v(x,y)来表示灰度值，公式为\nv(x,y) = ax + by + cxy + d\n双三次内插\n使用16个最近邻点，公式为:\nv(x,y) = ∑(3 i=0)∑(3 j=0)a(ij)x^i*y^j 双三次内插在保持细节方面比双线性内插要好，商业图像编辑软件标准内插方法，比如Adobe Photoshop\n在matplotlib.pylot.imshow的一个选项中,interpolation是选择内插方法\n未完待续","date":"2018-03-22T17:22:22Z","permalink":"https://kinokosu3.github.io/p/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/","title":"数字图像处理笔记"},{"content":"说是职业规划，其实仅仅只是为了进公司找到工作(对于升职加薪这些对于我这个在校生实在是太远了啊！！)，在CS界可是龙争虎斗的，特别是现在是大热的Data Scientist，更是血雨腥风。以下大部分内容来自知乎\n知乎，与世界分享你刚编的故事(笑)\n来，我们看第一个知乎里面关注较高的问题零基础自学如何成为合格的数据挖掘工程师:\n{%asset_img 1.png 1 %}\n如果对这个问题感兴趣，可以直接访问观看，以下同理。\n这个问题的第一个回答很务实，直接给的课程地址:\n{%asset_img 2.png 2 %}\n我真正感兴趣的是第二个问答:\n{%asset_img 3.png 3 %}\n内容较多，而且英文名词较多，简单讲一下:\n首先就是线性代数(Linear Algebra)\n因为无论是机器学习还是数据挖掘，大部分都是矩阵推导，python的numpy库和pandas库利用它们特有的数据结构简化了一部分。\n接下来是概率论和统计学\n当然不需要全部掌握，但是还是需要掌握其中的一部分知识，不然直接生啃里面的数学模型也够呛，一些特有的数学符号也需要认识。\n接下来就是数据挖掘和机器学习的专业知识了。\n这回答有句话讲的很好，如果说数学是战术，那么编程就是武器。\n编程语言我们就用Python了，这个毋庸置疑\n对于一些Big Data框架我们有，hadoop，sparak之类的，了解一下。\n看完上面的，是不是对这个工作产生了恐惧性？？？\n我们来看第二个帖子阿里巴巴数据挖掘工程师要求什么技能？:\n哪怕是同一领域的职位，因为部门的不同，所用的知识层面也是不同的，哪怕是BAT中的阿里\n{%asset_img 4.png 4%}\n最后我觉得第三个较为适合我们大家，这是这个问题其中一个回答机器学习、数据挖掘 如何进阶成为大神？:\n{%asset_img 5.png 5 %}\n{%asset_img 6.png 6 %}\n","date":"2018-03-18T10:39:46Z","permalink":"https://kinokosu3.github.io/p/datamining%E7%9A%84%E8%81%8C%E4%B8%9A%E8%A7%84%E5%88%92/","title":"DataMining的职业规划"},{"content":"前言 欢迎来到Python的世界！！\n也许你已经听过许多的计算机编程语言，比如C/C++,比如经久不衰且流行的Java,比如适合写网页的JavaScript等等\n你会很好奇，python和其他语言到底有什么不同，我们从开发效率来说，比如，完成同一个任务，C语言要写1000行代码，Java只需要写100行，而Python可能只要20行。\n所以Python是一种非常高级的语言。\n代码少，带来的是运行效率的低下，C程序运行1秒钟，Java程序可能需要2秒，而Python程序可能就需要10秒。\n虽然我并不想介入语言之争，但是在github 2017的年度报告，Python超过了Java成为了第二名，这从侧面体现出Python的流行程度，连BAT、Google等等这些公司都在使用Python,所以不必担忧学习了Python之后未来用不上。\nPython能干很多事情，日常任务，可以写网络游戏的后台，可以做网站，比如Youtube，就是使用Python写的\n当然，也有Python不能做的事情，比如操作系统，这个只能用C语言写(接近底层且快)，开发APP，只能用Swift/Objective-C(iphone)或者Java/Kotlin(Android)，写游戏内核(比如PUBG的虚幻4引擎)，最好使用C/C++\npython极其简单，哪怕你没有学过任何编程语言，会使用电脑，你只要上过初中的数学课，并且认真听了，就能学会这门编程语言。\n虽然这么说，但是在python的高级程序设计中，这些抽象的知识也是很难学的(比如元编程、鸭子类型、函数式编程)\n最后，人生苦短，我用Python。\n国外大佬的Python全指南(官方中文版)(甚至包括如何装Python)(requests库之父)Hitchhiker\u0026rsquo;s Guide to Python\n一个基本的学习计划(面向数据类) 一个月基本要掌握的内容:\n输入输出函数(input)(print)\n数据类型和变量\n字符串操作和编码\nif、for in 、while\npython基础数据结构的基本使用(dict)(list)(tuple)\nand or not 逻辑运算符的使用\n学会第三方库的导入\n(基于快速上手的原则，我们暂时不学习面向对象编程,如果学有余力可以学习)\n接触并使用函数\n文件的IO读写\n数据结构的高级特性(切片)(拆包)(生成式)\n错误捕捉\n？？？？？\n大概的可能就这些，基本的学习教程可以自己看书或者看在线教程\n我们可以练一些数学题来熟悉python，比如质数、水仙花数、公倍数之类的纯数学问题\n然后我们的第一个方向就是爬虫，这个我们就不细讲，我们需要学会的几个库(其实只需要学会其中的几种功能):\nlxml\nBeautifulSoup\nrequests\njson(标准库)\n然后学一下HTML的基本知识,比如:\n元素 属性 段落 再了解一下HTTP的基本知识:\nHTTP方法(get,post,put之类) URL cookie HTTP报文头部 学习一下chrome的检查元素(chrome/Firefox/Safari),了解网站的构造和服务器与网站如何交互\n对于动态加载的网站,我们了解一下AJAX、异步加载,然后我们可以直接抓取服务器APi传输的数据:\nbilibili新番页面(因为有播放量等的缘故，所以需要随时更新而使用ajax) {% asset_img XHR.png bilibili%}\n基本自己手写爬虫就没有任何问题了，如果想提高效率可以学习多线程或者协程(较难),如果嫌还是太麻烦，可以学习爬虫框架，比如scrapy.\n","date":"2018-03-17T10:57:22Z","permalink":"https://kinokosu3.github.io/p/python%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B0%E6%89%8B%E5%90%91/","title":"python学习计划(数据分析新手向)"},{"content":"前言 对于数据挖掘来说，并不都是巨量的数据集和复杂而抽象的数学模型，有的时候，我们也需要直观的图形来表示，对于文本的数据分析来说，词云wordcloud就是其中一种较为流行的可视化方式。下图就是Solidot3个月新闻的高频词的一个可视化。\n实例展示 TOP10 {%asset_img 10.png test%}\nTOP50 {%asset_img 50.png test%}\nTOP100 {%asset_img 100.png test%}\n中文分词简单实现 经过对比几个流行的python中文分词库，我选择了jieba\njieba主要模式 支持三种分词模式：\n精确模式，试图将句子最精确地切开，适合文本分析； 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义； 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。支持繁体分词 支持自定义词典 其他的功能可以进入该库的github页面学习jieba。\nTF-IDF算法的关键词的提取 关于该算法的解释: TF-IDF\n因为jieba集成了实现，所以我们可以很简单的使用这个算法进行高频词提取。\npython的使用:\nimport jieba.analyse # text_from_file是待提取的文本 # tok提取50个高频词(前top50) # withWeight 为是否一并返回关键词权重值，默认值为 False # tags为一个元组元素组成的列表，元素为关键词和关键词权重值。 tags = jieba.analyse.extract_tags(text_from_file, topK=50, withWeight=True) 停用词和词典的导入 如果你不是使用停用词和自定义的词典的话，你会发现像万一、亿美元之类的词会成为我们的高频词，一篇文章自然会出现这些量词之类的，而且会频繁出现，或者会把如云计算拆分成计算这个词，而我们需要的是里面出现的一些名词，这时候我们就需要停用词和自定义词典来更为准确的分词。\njieba.load_userdict(\u0026#34;userdict.txt\u0026#34;) jieba.analyse.set_stop_words(\u0026#39;stop_words.txt\u0026#39;) 停用词词典示例:\n{%asset_img stop_word.png stop_word%}\nWordCloud(词云)构建 主要参数 # from wordcloud import WordCloud # font_path为字体 # random_state 随机数种子 wc = WordCloud( font_path=\u0026#39;simkai.ttf\u0026#39;, background_color=\u0026#39;white\u0026#39;, max_words=50, max_font_size=200, random_state=42, width=1500, height=1500, ) 需要注意的是WordCloud如果不指定中文字体会无法显示\nwc.generate_from_frequencies(data_dict)根据词频生成WordCloud。参数为Python的字典类型。\n如果想使用matplotlib进行调试的话，我们这样使用:\nimport matplotlib.pyplot as plt plt.figure() plt.imshow(wc) plt.axis(\u0026#34;off\u0026#34;) plt.show() matplotlib显示的时候会比输出的图片有白边，是因为那个地方原来是坐标系，我们使用axis()方法屏蔽了\n保存图片 wc.to_file(\u0026#39;test.png\u0026#39;) 词云形状 其实还可以使用图片来生成词云的形状的，这个可以百度或者谷歌一下\n","date":"2018-03-16T16:49:14Z","permalink":"https://kinokosu3.github.io/p/%E8%AF%8D%E4%BA%91%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/","title":"词云简单实现"},{"content":"import cv2的坑(Ubuntu) 需要安装库apt-get install libsm6 libxext6 libxrender1\n","date":"2018-03-11T14:51:51Z","permalink":"https://kinokosu3.github.io/p/opencv-python%E7%9A%84%E5%9D%91/","title":"OpenCV-python的坑"},{"content":"前言 最近小组那边找了一大批新人，看起来都是大佬的样子(笑，而且我也要去上课之类的，所有只好重新开一台阿里云的服务器，上个学期那台已经被阿里云释放了，但是我依旧做好了备份，所以应该没有数据丢失(上个学期爬的100M豆瓣数据说丢就丢毫不含糊)，既然重新来，就做一个记录，当一个wiki使用。该文会长期更新。。\n服务器前期工作 因为安全问题和方便的问题，安全登录我们先做好准备，首先做好ssh的登录准备。\n生成ssh-key实现无密码登录 无论是本地还是远程，生成ssh秘钥都是一句命令:\nssh-keygen -t rsa -C \u0026#34;your email\u0026#34; 实现免密码登录只需要把本地.ssh/id_rsa.pub内的秘钥放在远程服务器的.ssh/authorized_keys内即可实现无密码登录。\nssh禁止密码登录 vim /etc/ssh/sshd_config 禁用密码验证 PasswordAuthentication no //修改为no 133行（最后一行）\n更新安装列表 apt-get update\n布置python环境 阿里云的Ubuntu系统自带两种稳定版python，一种是Python 2.7.12，一种是Python 3.5.2，但是我都不想用，我就是想用又新又稳定的，比如Python-3.6.4。\n我们在python官网上下载编译包wget https://www.python.org/ftp/python/3.6.4/Python-3.6.4.tgz\n然后解压tar -zxvf Python-3.6.4.tgz。\n我们首先先去编译安装一个zlib，不然安装pip和setuptool会出错 我们还需要安装openssl,libssl-dev, libsqlite3-dev 然后修改python-3.6.4/Modules/Setup，修改\n# Socket module helper for socket(2) _socket socketmodule.c # Socket module helper for SSL support; you must comment out the other # socket line above, and possibly edit the SSL variable: #SSL=/usr/local/ssl _ssl _ssl.c \\ -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \\ -L$(SSL)/lib -lssl -lcrypto ！！！！！！！！！！！！！一定要完成上面的注意事项再进行编译安装！！！！！！！！！！！！！！！\n我们进入刚刚解压出来的文件夹里，执行3连:\n./configure make sudo make install python虚拟环境virtualenv pip3 install virtualenv\n查看Python3.6安装路径:which pyhton3.6\n创建虚拟环境: virtualenv -p /usr/bin/python3 py3env\n进入虚拟环境: source py3env/bin/activate\n退出虚拟环境： deactivate\n布置ipython-notebook pip install ipython、pip install jupyter.\n启动时候注意端口和地址。\n","date":"2018-03-11T11:37:21Z","permalink":"https://kinokosu3.github.io/p/liunx%E5%85%A8%E5%85%BB%E6%88%90/","title":"liunx全养成①"},{"content":"前言 做数据分析首先需要数据集进行分析，但是我们没有大规模的数据集，可以采用公开的数据集，我们现在做的是一个关联分析的推荐系统，有一个非常出名的电影数据测试集- Moivelens.\n选择数据集 里面有很多的大小不同的数据集，我们选择最大的那个ml-20m。毕竟我们需要大数据才能复现算法。\n需要的python库 因为我们暂时不需要可视化，所以只引入了下面的库：\npandas (写到这里愣了一下，发现只用了一个数据分析库，pandas确实是python的数据分析神器)\n以下全部使用ipython-notebook运行，最后也会放出总的ipynb文件\n数据集整理 我们读取我们的数据集\nDir = \u0026#39;/Users/su/Desktop/python_project/ml-20m/ratings.csv\u0026#39; all_ratings = pd.read_csv(Dir) 然后输出发现数据集是以下的格式：\n这样的数据结构在pandas里面叫DataFrame，是一个表格型的数据结构，是左边的数字是行索引(index),还有列索引,我们一般都通过对访问索引来进行列的操作。\n最右边的timestamp列是时间戳，相比有:、.的公元纪时更容易存储和使用(只是一串数字)。\n时间戳的介绍Unix时间\npandas里有时间戳转换的方法to_datatime。\n# 时间戳转换 all_ratings[\u0026#39;timestamp\u0026#39;] = pd.to_datetime(all_ratings[\u0026#39;timestamp\u0026#39;], unit=\u0026#39;s\u0026#39;) 将代码运行后，重新将all_ratings输出：\n我们发现时间转换公元时间了。\nApriori算法复现 我们要实现的关联规则是\n如果一个用户喜欢一个电影，那么他们可能喜欢这部电影。 首先我们要知道，用户是否喜欢这一步电影，我们创建一个新的特征favorable,如果用户喜欢这部电影,值为True 。\n代码:\nall_ratings[\u0026#34;favorable\u0026#34;] = all_ratings[\u0026#39;rating\u0026#39;] \u0026gt; 3 我们看一下这个新的特征：\n我们可以先选一部分作为训练集,这样能提高训练速度。\n# 选取前200作为训练集 # isin是判断矢量化集合的成员资格，返回bool值,以下返回符合资格的成员，返回前200（符合） ratings = all_ratings[all_ratings[\u0026#39;userId\u0026#39;].isin(range(200))] 接下来我们新建一个数据集，只包括喜欢电影的数据列。\n# 返回一个打分全部高于3.0的dataframe favorable_ratings = ratings[ratings[\u0026#39;favorable\u0026#39;]] 我们生成频繁项集的时候会搜索用户喜欢的电影，所以我们需要知道每个用户喜欢的电影。我们按照userid分组，并遍历每个用户看过的每一部电影。\n# 分组运算 groupby # favorable_ratings.groupby(\u0026#34;userId\u0026#34;)[\u0026#34;movieId\u0026#34;]是 # favorable_ratings[\u0026#39;movieId\u0026#39;].groupby(favorable_ratings[\u0026#34;userId\u0026#34;])的语法糖 # k是分组名（用户名），v是数据块（电影id） # frozenset是不可变集合 favorable_reviews_by_users = dict((k, frozenset(v.values)) for k, v in favorable_ratings.groupby(\u0026#34;userId\u0026#34;)[\u0026#34;movieId\u0026#34;]) 为什么我们使用frozenset这种不可变集合，因为这种数据结构没有添加和删除操作，分配的内存小，速度很快，比列表(list)、可变集合(set)的速度都快。\n分组运算的原理可以看下图:\n我们做完之后我们看看效果：\n现在我们知道每个影迷所喜欢的电影了，然后我们再对每部电影拥有的影迷进行计数：\n# 获取影迷数量求和,对每部电影进行支持度计数（） num_favorable_by_movie = ratings[[\u0026#34;movieId\u0026#34;, \u0026#34;favorable\u0026#34;]].groupby(\u0026#34;movieId\u0026#34;).sum() 做了前面的准备之后到现在我们开始实现Apriori算法\n我们创建一个频繁项集，并且设定最小支持度。\nfrequent_itemsets = {} # 频繁项集 min_support = 50 # 最小支持度 我们开始生成初始的频繁项集，就是1维频繁项集，\nfrequent_itemsets[1] = dict((frozenset((movie_id,)), row[\u0026#39;favorable\u0026#39;]) for movie_id, row in num_favorable_by_movie.iterrows() if row[\u0026#34;favorable\u0026#34;] \u0026gt; 50) 无论是列表推导式和字典推导式在这次复现都会大量出现。\n我们创建下面的这样一个函数：\ndef find_frequent_itemsets(favorable_reviews_by_users, k_1_itemsets, min_support): counts = defaultdict(int) for user, reviews in favorable_reviews_by_users.items(): for itemset in k_1_itemsets: if itemset.issubset(reviews): for other_reviewed_movie in reviews - itemset: current_superset = itemset | frozenset((other_reviewed_movie,))） counts[current_superset] += 1 return dict([(itemset, frequency) for itemset, frequency in counts.items() if frequency \u0026gt;= min_support]) 这个函数的功能是传入一个k维的频繁项集，返回一个k+1维的频繁项集 。\n我们一步步对这个函数进行解析，传入3个参数,第一个是之前使用groupby分组运算分组出来的分组,第二个是k维项集,第三个是最小支持度。\ncounts = defaultdict(int) 生成了一个默认为0的带key的数据字典,values的值具有默认值，keys值自定。 for user, reviews in favorable_reviews_by_users.items(): 遍历影迷和他们喜欢的电影集合 for itemset in k_1_itemsets: if itemset.issubset(reviews): issubset是判断子集函数,判断itemset是否是reviews的子集,如果是则表示用户已经为该电影打分。 for other_reviewed_movie in reviews - itemset: current_superset = itemset | frozenset((other_reviewed_movie,)) counts[current_superset] += 1 遍历用户打过分却没有出现过在项集里面的电影,然后生成候选项集, itemset | frozenset((other_reviewed_movie,))是并集。然后我们进行支持度计数。counts[current_superset] += 1这样的集合出现一次我们就进行一次+1,支持度计数 。 cout这个数据字典上面有介绍,我们使用每个候选项集作为键,支持度为值。\nreturn dict([(itemset, frequency) for itemset, frequency in counts.items() if frequency \u0026gt;= min_support]) 最后在return函数检测频繁程度,返回其中的频繁项集。\n一个简单的k维for循环:\nfor k in range(2, 20): cur_frequent_itemsets = find_frequent_itemsets(favorable_reviews_by_users, frequent_itemsets[k-1], min_support=min_support) frequent_itemsets[k] = cur_frequent_itemsets 执行完后我们得到多个维的频繁项集,我们最多只发现了6维的频繁项集。\n我们把1维的项集去掉,因为生成关联规则需要两个项目。\ndel frequent_itemsets[1] 创建一个列表存储关联规则\ncandidate_rules = [] # 关联规则 我们遍历我们生成的频繁项集,第一层for循环把k维项集里面的频繁项集取出，第二层for循环获得每一个项集，第三层遍历项集内数据。\n我们使用前提和结论作为规则，结论仅仅只为一个值得推荐的电影。前提为频繁项集内的项目。\nfor itemset_length, itemset_counts in frequent_itemsets.items(): for itemset in itemset_counts.keys(): for conclusion in itemset: premise = itemset - set((conclusion,)) candidate_rules.append((premise, conclusion)) 我们输出我们的关联规则看看:\n获得关联规则后，我们还需要计算置信度，置信度为规则应验的次数。\n首先我们建两个defaultdict字典用来存储规则应验的次数。\n# 存储规则应验的次数 corrent_couts = defaultdict(int) # 不应验的次数 incorrect_couts = defaultdict(int) 开始计算置信度:\nfor user, reviews in favorable_reviews_by_users.items(): for candidate_rule in candidate_rules: premise, conclusion = candidate_rule # 判断是否喜欢前提电影 if premise.issubset(reviews): # 判断是否喜欢结论电影 if conclusion in reviews: corrent_couts[candidate_rule] += 1 else: incorrect_couts[candidate_rule] += 1 # 置信度计算 P(B|A) = P(AB) / P(A)，每条规则的置信度 rule_confidence = {candidate_rule: corrent_couts[candidate_rule] / float(corrent_couts[candidate_rule] + incorrect_couts[candidate_rule]) for candidate_rule in candidate_rules} 然后我们对置信度进行排序：\nsorted_confidence = sorted(rule_confidence.items(), key=itemgetter(1), reverse=True) sorted函数使用可以看这里sorted函数\nApriori算法的训练生成关联规则暂时到这里就结束了。接下来我们要用剩下来的从数据集作为测试集。\n评估关联规则 我们首先将关联规则置信度高的前5位输出:\nfor index in range(5): print(\u0026#34;规则 #{0}\u0026#34;.format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] print(\u0026#34;关联规则:如果一个人喜欢 {0} 他们应该也会喜欢 {1}\u0026#34;.format(premise, conclusion)) print(\u0026#34;- 置信度:{0:.3f}\u0026#34;.format(rule_confidence[(premise, conclusion)])) print(\u0026#39;\\n\u0026#39;) 我们使用之前的关联规则对剩下的数据集进行置信度排序:\ntest_dataset = all_ratings[~all_ratings[\u0026#39;userId\u0026#39;].isin(range(200))] test_favorable = test_dataset[test_dataset[\u0026#34;favorable\u0026#34;]] test_favorable_by_users = dict((k, frozenset(v.values)) for k, v in test_favorable.groupby(\u0026#34;userId\u0026#34;)[\u0026#34;movieId\u0026#34;]) correct_counts = defaultdict(int) incorrect_counts = defaultdict(int) for user, reviews in test_favorable_by_users.items(): for candidate_rule in candidate_rules: premise, conclusion = candidate_rule if premise.issubset(reviews): if conclusion in reviews: correct_counts[candidate_rule] += 1 else: incorrect_counts[candidate_rule] += 1 test_confidence = {candidate_rule: correct_counts[candidate_rule] / float(correct_counts[candidate_rule] + incorrect_counts[candidate_rule]) for candidate_rule in rule_confidence} sorted_test_confidence = sorted(test_confidence.items(), key=itemgetter(1), reverse=True) 获得测试集的置信度后，我们将前5条输出:\nfor index in range(5): print(\u0026#34;规则 #{0}\u0026#34;.format(index + 1)) (premise, conclusion) = sorted_confidence[index][0] print(\u0026#34;关联规则:如果一个人喜欢 {0} 他们应该也会喜欢 {1}\u0026#34;.format(premise, conclusion)) print(\u0026#34; - 训练集 置信度: {0:.3f}\u0026#34;.format(rule_confidence.get((premise, conclusion), -1))) print(\u0026#34; - 测试集 置信度: {0:.3f}\u0026#34;.format(test_confidence.get((premise, conclusion), -1))) print(\u0026#34;\u0026#34;) 未完待续 ","date":"2017-12-10T10:44:16Z","permalink":"https://kinokosu3.github.io/p/apriori-python%E5%A4%8D%E7%8E%B0%E8%BF%87%E7%A8%8B/","title":"Apriori-python复现过程"},{"content":"前言 最近也要布置一个爬虫项目到云端，之前一直都在本地测试，这次部署也暴露了很多问题，就记一下。\nmongodb部署 mongodb后台运行 我们可以使用配置文件进行运行，带参数-f。后台运行可以在配置文件里设置fork=true。\n下面贴出简单的配置文件：\ndbpath=/data/db/ # 数据库路径 logpath=/data/db/mongodb.conf # 输出日志路径 logappend=true\t# 日志添加 bind_ip = 0.0.0.0 # 设置可以外网访问 port = 27017 # mongodb默认端口 journal=true fork=true # 后台运行 关闭后台运行 我们可以使用命令ps -aux | grep mongod，找到mongod的PID，然后使用命令kill -2 PID安全关闭。\nmongodb警告解决 我们使用mongo shell链接mongod会发现有一些警告，比如这个：\n2015-10-21T09:03:24.256+0800 I CONTROL [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/enabled is \u0026#39;always\u0026#39;. 2015-10-21T09:03:24.256+0800 I CONTROL [initandlisten] ** We suggest setting it to \u0026#39;never\u0026#39; 2015-10-21T09:03:24.256+0800 I CONTROL [initandlisten] 2015-10-21T09:03:24.256+0800 I CONTROL [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/defrag is \u0026#39;always\u0026#39;. 2015-10-21T09:03:24.256+0800 I CONTROL [initandlisten] ** We suggest setting it to \u0026#39;never\u0026#39; 查了一下，好像对内存消耗很大的服务都不喜欢它。\n我们有几种方法可以关闭，可以这样：\necho never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled echo never\u0026gt; /sys/kernel/mm/transparent_hugepage/defrag 还可以永久关闭它,修改/etc/rc.local：\nif test -f /sys/kernel/mm/transparent_hugepage/enabled; then echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled fi if test -f /sys/kernel/mm/transparent_hugepage/defrag; then echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/defrag fi 还有一个问题一直不知道怎么解决，就是\n2017-11-19T17:13:57.343+0800 I CONTROL [initandlisten] ** WARNING: soft rlimits too low. rlimits set to 7857 processes, 65535 files. Number of processes should be at least 32767.5 : 0.5 times number of files. 明明修改了那个什么limits文件加了mongod 64000什么的都不行。\nmongodb用户权限管理 在mongo shell里命令有：\n# 创建用户 \u0026gt; db.createUser({ user:\u0026#34;\u0026lt;name\u0026gt;\u0026#34;, pwd:\u0026#34;\u0026lt;password\u0026gt;\u0026#34;, roles:[ role: \u0026#34;\u0026lt;role\u0026gt;\u0026#34;, db: \u0026#34;\u0026lt;database\u0026gt;\u0026#34; ] }) # 验证用户 \u0026gt; db.auth(\u0026lt;username\u0026gt;,\u0026lt;password\u0026gt;) # 修改用户密码 \u0026gt; db.changeUserPassword(\u0026lt;username\u0026gt;, \u0026lt;password\u0026gt;) # 删除用户 \u0026gt; db.dropUser(\u0026lt;username\u0026gt;, \u0026lt;password\u0026gt;) # 删除全部用户 \u0026gt; db.dropAllUsers() # 获取用户 \u0026gt; db.getUser(\u0026lt;username\u0026gt;) # 获取多个用户 \u0026gt; db.getUsers() # 更新用户资料 \u0026gt; db.revokeRolesFromUser() \u0026gt; db.revokeRolesFromUser( \u0026#34;accountUser01\u0026#34;, [ { role: \u0026#34;read\u0026#34;, db: \u0026#34;stock\u0026#34; }, \u0026#34;readWrite\u0026#34; ], { w: \u0026#34;majority\u0026#34; } ) root权限必须在admin数据库里验证。\npymongo里用户验证 client[\u0026#39;runoob\u0026#39;].authenticate(\u0026#39;scrapy\u0026#39;,\u0026#39;abc\u0026#39;,\u0026#39;runoob\u0026#39;,mechanism=\u0026#39;SCRAM-SHA-1\u0026#39;) ","date":"2017-11-19T20:27:08Z","permalink":"https://kinokosu3.github.io/p/mongodb%E4%B8%80%E4%BA%9B%E9%83%A8%E7%BD%B2%E8%AE%BE%E7%BD%AE/","title":"mongodb一些部署设置"},{"content":"前言 最近进了一个数据挖掘的小组，又是干爬虫的事，以前使用bs4+requests用的很顺，但是要优化的东西太多了，就打算学习个框架什么的，就看中了scrapy，希望它不错\n用了一段时间之后，只能感叹，还是轮子好用。\nscrapy基础开发 一个简单的项目 基础项目格式\n. ├── __init__.py ├── items.py ├── middlewares.py ├── pipelines.py ├── run.py ├── settings.py └── spiders ├── __init__.py ├── main_spider.py ├── my_spider.py ├── quotes_spider.py └── test_spider.py spiders文件夹里存放爬虫文件，settings是配置文件，items是字段文件，run是从文件执行爬虫，middlewares是中间件，piplines用来清理items或者存进数据库。\n一个简单的爬虫文件 我们的文件放在spiders文件夹里，一个简单的爬虫：\nnew_spiders.py:\nimport scrapy class TestSpider(scrapy.Spider): name = \u0026#34;test\u0026#34; start_urls = [ \u0026#39;https://www.bilibili.com/\u0026#39; ] def parse(self, response): yield {\u0026#34;title\u0026#34;: h3} name是爬虫的名字，start_urls是爬虫开始爬的第一个页面。一个爬虫一个爬虫类。\nparse方法是固定的，用来返回数据。\nscrapy shell调试 我们使用命令scrapy shell url，就可以进入这个url下的一个控制台\n一般我们使用返回的response进行当前网站的分析，使用response.xpath就可以定位元素。\n我们能做的：\n[s] scrapy scrapy module (contains scrapy.Request, scrapy.Selector, etc) [s] crawler \u0026lt;scrapy.crawler.Crawler object at 0x10068c320\u0026gt; [s] item {} [s] request \u0026lt;GET https://www.bilibili.com/\u0026gt; [s] response \u0026lt;200 https://www.bilibili.com/\u0026gt; [s] settings \u0026lt;scrapy.settings.Settings object at 0x10754eac8\u0026gt; [s] spider \u0026lt;DefaultSpider \u0026#39;default\u0026#39; at 0x1078eeda0\u0026gt; [s] Useful shortcuts: [s] fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed) [s] fetch(req) Fetch a scrapy.Request and update local objects [s] shelp() Shell help (print this help) [s] view(response) View response in a browser 选择器(Selectors) 我们通常使用xpath来解析网页。比如：\nresponse.xpath(\u0026#39;//title/text()\u0026#39;).extract() 我们调用extract()方法来获取原文数据，整句返回一个List。\n我们还能使用正则表达式，比如：\nresponse.xpath(\u0026#39;//title/text()\u0026#39;).re(\u0026#39;\\d+\u0026#39;) 不需要加extract()方法。\nItems scrapy的字段设置在项目里的items.py文件里设置。\nimport scrapy class FirstSpiderItem(scrapy.Item): # define the fields for your item here like: title = scrapy.Field() pass 设置很简单，所有的字段都设置成Field。\n使用Items也很简单，我们把字段导入：\nfrom ..items import DoubanBookStatusItem 在parse方法（也可以是自己定义的返回数据方法）里可以这样使用：\nitem = DoubanBookStatusItem() item[\u0026#39;press\u0026#39;] = response.xpath(\u0026#39;//*[@id=\u0026#34;info\u0026#34;]\u0026#39;).re(\u0026#39;(?\u0026lt;=出版社:\u0026lt;/span\u0026gt; ).*?(?=\u0026lt;br\u0026gt;)\u0026#39;)[0] # 出版社 press为我们定义的字段名。\n有一个需要注意的，我们使用迭代器yield返回的数据全部都是字典格式的。\npiplines 功能需要在settings里面设置\n我们使用mongodb保存,直接贴出代码\nclass MongoPipline(object): collection_name = \u0026#39;scrapy_items\u0026#39;\t# mongodb集合名 def __init__(self): self.mongo_server = \u0026#39;localhost\u0026#39; self.mongo_db = \u0026#39;runoob\u0026#39; self.port = 27017 self.client = None self.db = None def open_spider(self, spider): self.client = pymongo.MongoClient(self.mongo_server, self.port) self.db = self.client[self.mongo_db] def close_spider(self, spider): self.client.close() def process_item(self, item, spider): self.db[self.collection_name].insert_one(dict(item)) return item piplines大概能实现下面功能：\n定义一个Python类，然后实现方法process_item(self, item, spider)即可，返回一个字典或Item，或者抛出DropItem异常丢弃这个Item。\n或者还可以实现下面几个方法：\nopen_spider(self, spider) 蜘蛛打开的时执行 close_spider(self, spider) 蜘蛛关闭时执行 from_crawler(cls, crawler) 可访问核心组件比如配置和信号，并注册钩子函数到Scrapy中。 ","date":"2017-11-10T22:16:08Z","permalink":"https://kinokosu3.github.io/p/scrapy%E5%BC%80%E5%8F%91%E5%A4%87%E5%BF%98%E5%BD%95/","title":"scrapy开发备忘录"},{"content":"装饰器基础 函数即为对象 python中的一切都是对象，对象有什么特征呢？比如：\n你可以把它赋值给一个变量 你可以赋值它 你可以给它添加属性 你可以作为函数参数来传递它 \u0026gt;\u0026gt;\u0026gt; print(ObjectCreator) # 你可以打印一个类,因为它是一个对象 \u0026lt;class \u0026#39;__main__.ObjectCreator\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; def echo(o): ... print(o) ... \u0026gt;\u0026gt;\u0026gt; echo(ObjectCreator) # 你可以把类作为参数传递 \u0026lt;class \u0026#39;__main__.ObjectCreator\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; print(hasattr(ObjectCreator, \u0026#39;new_attribute\u0026#39;)) False \u0026gt;\u0026gt;\u0026gt; ObjectCreator.new_attribute = \u0026#39;foo\u0026#39; # 可以给一个类添加属性 \u0026gt;\u0026gt;\u0026gt; print(hasattr(ObjectCreator, \u0026#39;new_attribute\u0026#39;)) True \u0026gt;\u0026gt;\u0026gt; print(ObjectCreator.new_attribute) foo \u0026gt;\u0026gt;\u0026gt; ObjectCreatorMirror = ObjectCreator # 可以把类赋值给一个变量 \u0026gt;\u0026gt;\u0026gt; print(ObjectCreatorMirror.new_attribute) foo \u0026gt;\u0026gt;\u0026gt; print(ObjectCreatorMirror()) \u0026lt;__main__.ObjectCreator object at 0x8997b4c\u0026gt; 函数还能在别的函数里定义，函数还能返回另外一个函数。\n实现装饰器 实现一个最简单的装饰器 一个小例子：\n# 其他函数作为参数 def my_decorator(func): def the_wrapper_fuction(): # 在原始函数被调用前的代码放在这里 print(\u0026#34;123\u0026#34;) func() return the_wrapper_fuction @my_decorator def func(): print(\u0026#34;main\u0026#34;) \u0026gt;\u0026gt;\u0026gt; 123 \u0026gt;\u0026gt;\u0026gt; main 装饰器高级用法 ","date":"2017-10-25T10:01:14Z","permalink":"https://kinokosu3.github.io/p/python%E8%A3%85%E9%A5%B0%E5%99%A8/","title":"python装饰器"},{"content":"特殊方法使用 __len__的使用 比如这样:\nclass FrenchDeck(object): def __init__(): self.__card = [1,2,3] def __len__(self): return len(self.__card) \u0026gt;\u0026gt;\u0026gt; deck = FrenchDeck() \u0026gt;\u0026gt;\u0026gt; len(deck) 3 直接获取长度，不用做过多的解释。\n列表推导和生成器表达式 列表推导 例子：\n\u0026gt;\u0026gt;\u0026gt; sym = \u0026#39;$#%\u0026amp;*\u0026#39; \u0026gt;\u0026gt;\u0026gt; codes=[] \u0026gt;\u0026gt;\u0026gt; codes = [ord(symbol) for symbol in sym] \u0026gt;\u0026gt;\u0026gt; codes [36, 35, 37, 38, 42] 一个标准的列表推导，我们不必担心换行的问题，我们甚至可以这样使用：\n\u0026gt;\u0026gt;\u0026gt; codes = [ord(symbol, id) for symbol in sym for id in range(0,5)] 生成器推导 这个和列表推导式的格式很像，仅仅把方括号换成圆括号。\n生成器推导可以逐个产生元素，而不是直接产生一个完整的列表。\n我们使用生成器表达式计算笛卡尔积\n例子：\n\u0026gt;\u0026gt;\u0026gt; for tshirt in (\u0026#39;%s %s\u0026#39; % (c,s) for c in colors for s in sizes): ... print(tshirt) ... black S black M black L white S white M white L 总结 列表生成式是直接生成一个完整的列表，而生成器推导则是可以逐个产生元素\n一个例子：\n\u0026gt;\u0026gt;\u0026gt; for o in (ord(symbol) for symbol in sym): ... print(o) ... 36 35 37 38 42 \u0026gt;\u0026gt;\u0026gt; [ord(symbol) for symbol in sym] [36, 35, 37, 38, 42] \u0026gt;\u0026gt;\u0026gt; type([ord(symbol) for symbol in sym]) \u0026lt;class \u0026#39;list\u0026#39;\u0026gt; \u0026gt;\u0026gt;\u0026gt; type(ord(symbol) for symbol in sym) \u0026lt;class \u0026#39;generator\u0026#39;\u0026gt; ","date":"2017-10-18T11:34:39Z","permalink":"https://kinokosu3.github.io/p/python%E7%89%B9%E6%AE%8A%E6%96%B9%E6%B3%95%E5%AD%A6%E4%B9%A0/","title":"python特殊方法学习①"},{"content":"前言 因为这段时间要去学校机房做实验，然后学校机房的网络可能经过了过滤，大陆外的ip一律无法访问，我们在机房用的系统是Ubuntu17.04，我直接用手机usb直接分享网络给机器使用（本来我还以为要使用复杂的网络设置，没想到现在的系统这么高级直接插上就可以使用了）。。但是我们还需要在liunx系统上进行外网访问，然后我测试了很多方法，比如以前的ss-Qt5，electron-ssr之类的GUI软件，但是都不行，后来有dalao指点，使用shadowsocksR-libv或者shadowsocksR-Python版加privoxy进行全局代理，然后我想了下，就试试吧，折腾了一晚终于完成了。\n最后的shadowsocksR-libv的版本是2.6.3\n配置 因为破娃妹子已经把源项目全部rm了，我们需要去shadowsocksR-backup里找到备份shadowsocksr-libev，然后git clone或者直接下载回到本地。\n我们可以来到文档处，因为我们是ubuntu系统我就进入ubuntu的配置\n输入命令\n$ cd shadowsocks-libev $ sudo apt-get install --no-install-recommends build-essential autoconf libtool libssl-dev \\ gawk debhelper dh-systemd init-system-helpers pkg-config asciidoc xmlto apg libpcre3-dev $ dpkg-buildpackage -b -us -uc -i $ cd .. $ sudo dpkg -i shadowsocks-libev*.deb 最后的shadowsocks-libev*.deb是最后打包好的deb包，输入打包出来的文件名。\n然后终极大坑来了\n我们使用命令ss-local使用代理。\n$ ss-local -o [obfs混淆协议] -O [protocol协议] -g [obfsparam混淆参数] -s [服务器地址] -p [服务器端口] -l [本地监听端口] -b [本地监听地址] -k [SS密码] -m [加密方式] 记住所有选项的顺序一定要按照以上顺序输入\n有个巨坑，我在测试的时候发现重启一次电脑之后，ss-local就没有混淆协议的选项了！！！！如果你使用-O、-g,ss-local会报错没有该参数，后来我转移到另外一台ubuntu虚拟机又可以使用了，坑死。。。。推荐如果出现错误，直接把shadowsocksR-libev删除掉，重新安装包。\n然后我们安装privoxy\n$ sudo apt-get install privoxy 安装完成之后，我们进入/etc/orivoxy更改config，把这句注释掉：\n# listen-address localhost:8118 然后在最后一行加上：\nforward-socks5 / 127.0.0.1:1080 . listen-address 127.0.0.1:8118 记住第一行后面有一个.，我配置的时候没有记得加.，然后一直不行吓死我了。\n然后我们配置全局的http proxy, sudo vim /etc/environment，加入下面的代码\nexport http_proxy=http://127.0.0.1:8118 export https_proxy=http://127.0.0.1:8118 然后我们重启计算机，重新来一次ss-local连上代理，使用curl www.google.com试试\n大功告成。\n","date":"2017-10-18T09:32:05Z","permalink":"https://kinokosu3.github.io/p/shadowsocsr-privoxy-liunx%E4%B8%8B%E4%BD%BF%E7%94%A8ssr%E5%85%A8%E5%B1%80%E4%BB%A3%E7%90%86/","title":"shadowsocsR+privoxy liunx下使用SSR全局代理"},{"content":"前言 哇，之前写一个flask项目的时候，一直打算部署，看了不少的教程，最后选择了flask+uwsgi+nginx的搭配，然后在Ubuntu server里尝试上线，但是就是不行，一直报错502，一直找不到问题所在，后来在家里的一台烂liunx笔记本尝试上线（一样是Ubuntu），这已经距离我的flask项目写好过了几个月了，然后一次就成功了，一脸蒙蔽，这个教程并没有太多的对于uwsgi和nginx的性能优化之类的，仅仅保留了最基本的配置。不会对配置做过多的解释，如果想知道配置文件的解释，可以自己谷歌或者百度查看uwsgi和nginx的配置文件解释。仅仅做一个备忘录。\n配置 nginx配置 nginx配置文件路径在/etc/nginx\nnginx配置文件如下：\nserver{ listen 80; server_name localhost; charset utf-8; keepalive_timeout 0; client_max_body_size 75M; location / { include uwsgi_params; uwsgi_pass unix:/home/su/upload_file-master/demo.sock; } } server_name为服务器地址，这里因为是本地上线，所以填了本地的地址，这里一个很主要的和uwsgi交互的地方在与uwsgi_pass这个配置选项，链接uwsgi的socket地址，生成这个socket的路径会在uwsgi的配置文件里配置。可以看下面的uwsgi配置。\n这个文件的名字是nginx.conf，尽量不要修改，我们将这个文件放在/etc/nginx/site-enabled/里，然后修改一下/etc/nginx/nginx.conf里的include /etc/nginx/sites-enabled/*.conf这一项。\n然后我们检查一下nginx的配置文件正确性，输入命令sudo nginx -t，如果显示:\n就表示nginx配置文件配置正确\nuwsgi 配置 建议将uwsgi配置文件放在项目根目录下，后缀为.ini\nuwsgi配置\n[uwsgi] processes = 8 wsgi-file = /home/su/upload_file-master/demo.py socket = /home/su/upload_file-master/demo.sock chmod-socket = 666 module = demo callable = app wsgi.file文件为flask的服务器入口，比如哪个文件有像这样的app.run(debug=True, port=9090, host=\u0026quot;0.0.0.0\u0026quot;)（其实就是服务器启动），socket的保存路径就是nginx配置文件中uwsgi_pass的路径，这个路径一定要对。chomd-socket把socket的权限改写成所有人可写可读。\ncallable选项为flask中的app = Flask(__name__)\n","date":"2017-10-11T22:39:05Z","permalink":"https://kinokosu3.github.io/p/flask-uwsgi-nginx%E7%9A%84%E9%83%A8%E7%BD%B2/","title":"flask+uwsgi+nginx的部署"},{"content":"运行环境配置 我们需要mingw和Cmake，这个是clion需要的运行环境\nmingw安装 百度mingw，然后点击download/installer,就会自动跳转到下载页面，然后安装，点击之后，一路continue，有个安装路径自己选择一下，如果选择其他路径，可以先创建一个空文件夹再选择其安装。\n然后会出现下载安装器的界面，安装完成之后继续按continue退出。 然后会跳转到这个界面 我们在要安装的选项右键之后点击Make for Intallation，然后选择以下的选项: 然后安装，在左上的Installation选项卡里选择，Apply Changes,就可以安装成功了\nCmake安装 我们在官网找到安装页面Download\n找到cmake 3.8.2的下载的地方，(其他的版本clion不认我也不知道为什么)，选择适合自己系统的版本下载，我们是windows，就选择windows的下载好了。\n然后一路安装就OK了\n","date":"2017-10-10T18:55:41Z","permalink":"https://kinokosu3.github.io/p/win%E4%B8%8B%E7%9A%84clion%E9%85%8D%E7%BD%AE/","title":"win下的Clion配置"},{"content":"开始React的世界 配置React 首先我们要安装Node.js环境，然后有npm安装环境，然后我们在命令行里执行\n$ npm install create-react-app 然后我们就会有一个React脚手架，可以迅速构建React开发环境\n$ npm install -g create-react-app $ create-react-app my-app $ cd myapp/ $ npm start 因为npm在国内很慢。。推荐在第一次创建项目的时候挂个科学上网什么的\n然后我们会看到我们的第一个demo\nMaterial-UI 使用教程 我们在当前项目的package.json中修改：\n\u0026#34;dependencies\u0026#34;: { \u0026#34;material-ui\u0026#34;: \u0026#34;^0.19.1\u0026#34;, \u0026#34;react\u0026#34;: \u0026#34;^15.6.1\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^15.6.1\u0026#34;, \u0026#34;react-scripts\u0026#34;: \u0026#34;1.0.13\u0026#34; }, 将material-ui添加进来,然后执行npm update --scripts-prepend-node-path=auto,将material-ui包添加进当前脚手架\n然后我们开始构建第一个demo。\n","date":"2017-09-17T16:24:19Z","permalink":"https://kinokosu3.github.io/p/react%E5%AD%A6%E4%B9%A0%E7%BA%AA%E5%BD%95/","title":"React学习纪录"},{"content":"qshell的使用 qshell安装 在这里下载\nqshell下载链接\n选择自己平台的文件下载：\n如果在使用的时候，报错Permission Denied,可以使用命令chmod +x qshell添加执行权限。 如果是liunx或者osx用户，可以添加PATH来直接启动\n添加或者显示当前用户的AccessKey和SecretKey 秘钥可以在个人中心查看\n命令格式qshell account [AccessKey] [SecretKey]\n上传文件 命令格式 qshell fput [空间名] [空间文件名] [文件路径]\n","date":"2017-09-06T16:41:54Z","permalink":"https://kinokosu3.github.io/p/%E4%B8%83%E7%89%9B%E4%BA%91api%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/","title":"七牛云API工具使用"},{"content":" 在monogo shell的命令使用 连接数据库 本地数据库链接，默认的路径是/data/db/,可以使用--dbpath来指定数据库路径\n$ sudo mongod --path ~/Desktop/db/ \u0026lt;The rest of contents | 余下全文\u0026gt;\n创建数据库 语法：use DATABASE_NAME\n也有切换数据库操作的意思\n\u0026gt; use runoob switched to db runoob \u0026gt; db runoob 查看全部数据库 \u0026gt; show dbs dmin 0.000GB local 0.000GB runoob 0.000GB 删除数据库 语法：db.dropDatabase()\n\u0026gt; db.dropDatabase() { \u0026#34;dropped\u0026#34; : \u0026#34;runoob\u0026#34;, \u0026#34;ok\u0026#34; : 1 } 创建集合 语法：db.createCollection(name,option)\noptions参数，类型是Document,可选有字段capped, autoIndexId,size,max\n\u0026gt; db.createCollection(\u0026#34;firstcollection\u0026#34;) 带有option参数的：\n\u0026gt; db.createCollection(\u0026#34;firstcollection\u0026#34;,{capped:ture}) 一般情况下不用主动创建集合，插入文档的参数中有collection_name\n删除数据库中的一个集合 语法：db.collection.drop()\n比如切换到runoob数据库删除里面的一个site集合\n\u0026gt; use runoob switched to db runoob \u0026gt; show tables site \u0026gt; db.site.drop() true \u0026gt; show tables \u0026gt; 插入文档 Mongodb文档的数据结构和json基本一样，所有的数据都是BSON(存储在集合中的)\n语法：db.COLLECTION_NAME.insert(document)\n更新文档 有update()和save()方法来更新集合中的文档\n语法格式如下：\n\u0026gt; db.collection.update( \u0026lt;query\u0026gt;, \u0026lt;update\u0026gt;, { upsert:\u0026lt;boolean\u0026gt;, multi:\u0026lt;boolean\u0026gt;, writeConcern:\u0026lt;document\u0026gt; } ) 参数说明：\nquery : update的查询条件，类似sql update查询内where后面的。\nupdate : update的对象和一些更新的操作符（如$,$inc\u0026hellip;）等，也可以理解为sql update查询内set后面的\nupsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。\nmulti : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。\n比如我们在runoob插入文档\n\u0026gt; db.runoob.insert({title:\u0026#39;MongiDB\u0026#39;,description:\u0026#34;Nosql\u0026#34;}) 然后我们通过update()来更新标题\n\u0026gt; db.runoob.update({\u0026#39;title\u0026#39;:\u0026#39;MongoDB\u0026#39;},{$set:{\u0026#39;title\u0026#39;:\u0026#39;Mongo\u0026#39;}}) 以上的语句只会更新找到的第一个文档，如果你要更新多个同样的文档，你需要把multi参数打开\n\u0026gt; db.runoob.update({\u0026#39;title\u0026#39;:\u0026#39;MongoDB\u0026#39;},{$set:{\u0026#39;title\u0026#39;:\u0026#39;Mongo\u0026#39;},{multi:ture}}) 接下来我们使用save()来更新文档，save()方法通过传递的文档来替换已有文档。\n语法格式如下：\ndb.collection.save( \u0026lt;document\u0026gt;, { writeConcern:\u0026lt;document\u0026gt; } ) 比如我们替换了_id为599ea3130a0004f919342ea5的文档数据\n\u0026gt; db.runoob.save({\u0026#34;_id\u0026#34;:ObjectId(\u0026#34;599ea3130a0004f919342ea5\u0026#34;), \u0026#34;name\u0026#34;:\u0026#34;kinoko3\u0026#34;}) MongoEngine使用 定义一个文档 from mongoengine import * import datetime class Page(Document): title = StringField(max_length=200, required=True) date_modified = DateTimeField(default=datetime.datetime.now) 保存到数据库 if __name__ == \u0026#39;__main__\u0026#39;: # 第一种保存方法 page = Page() page.title = \u0026#34;Hatsune Miku\u0026#34; page.save() # 第二种保存方法 page_1 = Page(title=\u0026#34;Hatsune Miku\u0026#34;) page_1.save() 保存之后mongoenine将会创建一个新的以class名命名的集合，比如class Page(Document)的一个类，创建的集合将会是page,查看创建的集合可以在mongodb shell里使用命令show tables或者show collections查看\n动态文档（DynamicDocument） 动态文档可以在保存数据的时候给数据自定不同的字段 比如：\nclass Page(DynamicDocument): title = StringField(max_length=200) if __name__ == \u0026#39;__main__\u0026#39;: page = Page(title=\u0026#39;Hatsune Miku\u0026#39;) page.tags = [\u0026#39;mongodb\u0026#39;, \u0026#39;mongoengine\u0026#39;] page.save() page.tags就是重新在这个文档里插入一个新的字段数据，在保存的时候，这个字段是之前没有在文档类里声明的，如果你不继承动态文档，直接在保存的时候添加字段，运行的时候不会报错，但是新加入的字段不会保存.上面的文档在mongodb shell中显示：\n\u0026gt; { \u0026#34;_id\u0026#34; : ObjectId(\u0026#34;59a2b15c6f1d6513f3401668\u0026#34;), \u0026#34;name\u0026#34; : \u0026#34;Hatsune Miku\u0026#34;, \u0026#34;tags\u0026#34; : [ \u0026#34;mongodb\u0026#34;, \u0026#34;mongoengine\u0026#34; ] } \u0026gt; tags字段是我们后来添加的。\n","date":"2017-08-25T19:14:07Z","permalink":"https://kinokosu3.github.io/p/mongodb%E5%A4%87%E5%BF%98%E5%BD%95/","title":"mongodb备忘录"},{"content":" 安装zsh 如果你是mac用户，系统已经自带有了，可以通过命令cat /etc/shells查看\n其他的liunx系统就从软件源安装就好了，Ubuntu，debianapt-get install zsh, centos,redhatyum apt-get zsh\n安装oh-my-zsh 虽然zsh功能强大，但是配置复杂，如果你专研过算法导论编译原理可以自己写一套。。。。所以国外有大神写了一套配置oh-my-zsh\ngit clone安装方法:\n克隆仓库 $ git clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh 创建一个新的zsh配置文件 $ cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc 改变默认的shell $ chsh -s /bin/zsh 重新打开终端就好了\npip安装powerline-status pip install powerline-status 使用的powerline-themes github链接oh-my-zsh-powerline-theme\n将文件中的powerline.zsh-theme复制到路径~/.oh-my-zsh/themes\n然后更改文件vim .zshrc,更改：\nZSH_THEME=\u0026#34;powerline\u0026#34; 添加：\nexport TERM=\u0026#34;xterm-256color\u0026#34; powerline字体 github链接链接\n然后安装powerline字体\n使用iterm2导入字体 preferences-\u0026gt;profiles-\u0026gt;colors-\u0026gt;color presets 导入主题颜色\ntext有字体选择，change font，选择powerline字体\nvim powerline开启 打开vim .vimrc\nset rtp+=/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/powerline/bindings/vim/ set laststatus=2 set t_Co=256 rtp+=后面跟的路径是在python的库文件夹site-packages里面，我们可以在交互模式下输入import site; site.getsitepackages()得到路径\nzsh插件开启 修改.zshrc\npulgins(\u0026lt;插件1\u0026gt;, \u0026lt;插件2\u0026gt;) 自动补全插件incr安装 官网链接\n在~/.oh-my-ssh/plugins/新建一个目录incr(注意要使用sudo创建)，然后新建一个文件sudo vim incr-0.2.zsh,把插件内容全部复制进去，然后配置.zshrc文件，在文档最后面插入\nsource ~/.oh-my-zsh/plugins/incr/incr*.zsh 防止某天插件链接挂了，下面放出插件内容\n# Incremental completion for zsh # by y.fujii \u0026lt;y-fujii at mimosa-pudica.net\u0026gt;, public domain autoload -U compinit zle -N self-insert self-insert-incr zle -N vi-cmd-mode-incr zle -N vi-backward-delete-char-incr zle -N backward-delete-char-incr zle -N expand-or-complete-prefix-incr compinit bindkey -M viins \u0026#39;^[\u0026#39; vi-cmd-mode-incr bindkey -M viins \u0026#39;^h\u0026#39; vi-backward-delete-char-incr bindkey -M viins \u0026#39;^?\u0026#39; vi-backward-delete-char-incr bindkey -M viins \u0026#39;^i\u0026#39; expand-or-complete-prefix-incr bindkey -M emacs \u0026#39;^h\u0026#39; backward-delete-char-incr bindkey -M emacs \u0026#39;^?\u0026#39; backward-delete-char-incr bindkey -M emacs \u0026#39;^i\u0026#39; expand-or-complete-prefix-incr unsetopt automenu compdef -d scp compdef -d tar compdef -d make compdef -d java compdef -d svn compdef -d cvs # TODO: # cp dir/ now_predict=0 function limit-completion { if ((compstate[nmatches] \u0026lt;= 1)); then zle -M \u0026#34;\u0026#34; elif ((compstate[list_lines] \u0026gt; 6)); then compstate[list]=\u0026#34;\u0026#34; zle -M \u0026#34;too many matches.\u0026#34; fi } function correct-prediction { if ((now_predict == 1)); then if [[ \u0026#34;$BUFFER\u0026#34; != \u0026#34;$buffer_prd\u0026#34; ]] || ((CURSOR != cursor_org)); then now_predict=0 fi fi } function remove-prediction { if ((now_predict == 1)); then BUFFER=\u0026#34;$buffer_org\u0026#34; now_predict=0 fi } function show-prediction { # assert(now_predict == 0) if ((PENDING == 0)) \u0026amp;\u0026amp; ((CURSOR \u0026gt; 1)) \u0026amp;\u0026amp; [[ \u0026#34;$PREBUFFER\u0026#34; == \u0026#34;\u0026#34; ]] \u0026amp;\u0026amp; [[ \u0026#34;$BUFFER[CURSOR]\u0026#34; != \u0026#34; \u0026#34; ]] then cursor_org=\u0026#34;$CURSOR\u0026#34; buffer_org=\u0026#34;$BUFFER\u0026#34; comppostfuncs=(limit-completion) zle complete-word cursor_prd=\u0026#34;$CURSOR\u0026#34; buffer_prd=\u0026#34;$BUFFER\u0026#34; if [[ \u0026#34;$buffer_org[1,cursor_org]\u0026#34; == \u0026#34;$buffer_prd[1,cursor_org]\u0026#34; ]]; then CURSOR=\u0026#34;$cursor_org\u0026#34; if [[ \u0026#34;$buffer_org\u0026#34; != \u0026#34;$buffer_prd\u0026#34; ]] || ((cursor_org != cursor_prd)); then now_predict=1 fi else BUFFER=\u0026#34;$buffer_org\u0026#34; CURSOR=\u0026#34;$cursor_org\u0026#34; fi echo -n \u0026#34;\\e[32m\u0026#34; else zle -M \u0026#34;\u0026#34; fi } function preexec { echo -n \u0026#34;\\e[39m\u0026#34; } function vi-cmd-mode-incr { correct-prediction remove-prediction zle vi-cmd-mode } function self-insert-incr { correct-prediction remove-prediction if zle .self-insert; then show-prediction fi } function vi-backward-delete-char-incr { correct-prediction remove-prediction if zle vi-backward-delete-char; then show-prediction fi } function backward-delete-char-incr { correct-prediction remove-prediction if zle backward-delete-char; then show-prediction fi } function expand-or-complete-prefix-incr { correct-prediction if ((now_predict == 1)); then CURSOR=\u0026#34;$cursor_prd\u0026#34; now_predict=0 comppostfuncs=(limit-completion) zle list-choices else remove-prediction zle expand-or-complete-prefix fi } 更多的选择（翻译）（oh-my-zsh-powerline-theme） 所有的选项都必须在你的.zshrc配置文件中修改\n默认下，在powerline右侧显示日期和时间，如果你不想要时间，可以选择你想要的\nPOWERLINE_RIGHT_B=\u0026#34;time replacement\u0026#34; 或者你不想显示任何内容：\nPOWERLINE_RIGHT_B=\u0026#34;none\u0026#34; 如果你要在时间旁边显示日期：\nPOWERLINE_RIGHT_A=\u0026#34;date\u0026#34; 如果你想在时间之前显示最后一个命令的退出代码\nPOWERLINE_RIGHT_A=\u0026#34;exit-status\u0026#34; 或者你想要它（退出代码）只出现在错误运行中：\nPOWERLINE_RIGHT_A=\u0026#34;exit-status-on-fail\u0026#34; 如果你要在时间之前显示上一个命令的日期或者非零退出代码\nPOWERLINE_RIGHT_A=\u0026#34;mixed\u0026#34; 如果你要在时间之后显示自定义文本\nPOWERLINE_RIGHT_A=\u0026#34;blackfire.io\u0026#34; 如果你想改变POWERLINE_RIGHT_A元素的颜色:\nPOWERLINE_RIGHT_A_COLOR_FRONT=\u0026#34;black\u0026#34; POWERLINE_RIGHT_A_COLOR_BACK=\u0026#34;red\u0026#34; 不翻了其实都看得懂。。。\n","date":"2017-08-19T19:16:41Z","permalink":"https://kinokosu3.github.io/p/zsh%E7%BB%88%E7%AB%AF%E6%8A%98%E8%85%BE%E7%BA%AA%E5%BD%95/","title":"zsh终端折腾纪录①"},{"content":" 基础命令（频繁使用） 查看镜像docker images\n构建镜像docker build -t\n-t是标签的意思，格式是\u0026lt;REPOSITORY \u0026gt; : \u0026lt;TAG\u0026gt;，如果你不创建这个，就会创建一个你FROM的镜像的默认名字，比如你在dockerfile指定FROM nginx，然后再docker build时候没有指定-t,你的这个镜像就会变成nginx:latest，你之前的nginx:latest就会变成nginx:\u0026lt;none\u0026gt;,所以构建的时候尽量指定-t.\n删除镜像 docker rmi ,后面可以跟镜像名:TAG的格式，也可以用IMAGES ID.\n运行容器 docker run\n我们看到很多时候需要交互模式，其中-it是交互模式的参数\n如果我们需要后台运行，参数-d\n--name \u0026lt;容器名\u0026gt;如果不使用这条参数，docker将会自己分配容器名\n停止容器docker stop，删除容器docker rm\n查看在运行的容器docker ps\n查看所有的容器docker ps -aq\n查看所有新创建的容器docker ps -l\n有时候我们需要一下子清理掉全部运行中的容器:\n$ docker stop $(docker ps -q) 然后删除掉全部容器：\n$ docker stop $(docker ps -aq) 一条命令实现：\n$ docker stop $(docker ps -q) \u0026amp; docker rm $(docker ps -aq) 放一张图，docker command，（图出自原图链接）\n","date":"2017-08-18T10:03:14Z","permalink":"https://kinokosu3.github.io/p/docker%E5%AE%9E%E8%B7%B5%E5%B0%9D%E8%AF%95/","title":"docker实践尝试"},{"content":" 生成requirement文件 在当前目录生成requirements文件\n$ pip freeze \u0026gt; requirements.txt 从requirements安装库\n$ pip install -r requirements.txt pip安装时有一个不保留缓存的参数--no-cache-dir\n找到site-packages文件夹 因为我用brew安装的python3，所以那个路径深似海啊，里面存放了我们的pip安装的一些第三方库\n我们可以在python交互模式里面输入\nimport site; site.getsitepackages() 就出来路径了\n","date":"2017-08-18T09:24:28Z","permalink":"https://kinokosu3.github.io/p/python%E7%9F%A5%E8%AF%86%E5%A4%87%E5%BF%98%E5%BD%95/","title":"python知识备忘录"},{"content":" ENV 设置环境变量 格式有两种\nENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; ENV \u0026lt;key1\u0026gt;=\u0026lt;value1\u0026gt; \u0026lt;key2\u0026gt;=\u0026lt;value2\u0026gt; RUN中使用环境变量记得加$\nEXPOSE 声明端口 格式为 EXPOSE \u0026lt;端口1\u0026gt; [\u0026lt;端口2\u0026gt;] 这只是一个声明，并不会因为运行就会开启这个端口的服务\n在运行时候使用-p \u0026lt;宿主端口\u0026gt;:\u0026lt;容器端口\u0026gt;，这个是在运行时使用的，-p是映射宿主端口和容器端口，而EXPOSE仅仅是声明打算使用什么端口而已，并不会进行端口映射\nWORKDIR 指定工作目录 格式为 WORKDIR \u0026lt;工作目录路径\u0026gt;\n改变每一层的工作目录，而不是用cd USER 指定当前用户 暂缺\n总结 Dockerfile是用来构建docker镜像的，每一个RUN为一层存储\n","date":"2017-08-17T10:13:31Z","permalink":"https://kinokosu3.github.io/p/docker%E5%AD%A6%E4%B9%A0%E7%BA%AA%E5%BD%95/","title":"Docker学习纪录③"},{"content":" python判断字符串是否包含子字符串的方法 方法①：in方法 @logging site = \u0026#34;www.baidu.com\u0026#34; if \u0026#34;baidu\u0026#34; in site: print(\u0026#39;site contains baidu\u0026#39;) 输出\u0026rsquo;site contains baidu'.\n方法②：字符串find函数 s = \u0026#34;www.baidu.com\u0026#34; if s.find(\u0026#34;baidu\u0026#34;) == -1: print(\u0026#34;no\u0026#34;) else: print(\u0026#34;have\u0026#34;) 这个find函数是搜索子字符串是否在母字符串中，不在返回-1，在则返回索引值\n","date":"2017-08-16T13:42:55Z","permalink":"https://kinokosu3.github.io/p/python%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8A%80%E5%B7%A7/","title":"python字符串技巧"},{"content":" Dockerfile定制镜像 大量引用Docker\u0026ndash;从入门到实践的内容\nGitbook地址：Docker\u0026ndash;从入门到实践\nFROM指定基础镜像 FROM nginx 指定一个nginx基础镜像\nRUN执行命令 shell格式 exec格式 dockerfile一条指令建立一层，RUN指令应该这么写\nFROM debian:jessie RUN buildDeps=\u0026#39;gcc libc6-dev make\u0026#39; \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; apt-get install -y $buildDeps \\ \u0026amp;\u0026amp; wget -O redis.tar.gz \u0026#34;http://download.redis.io/releases/redis-3.2.5.tar.gz\u0026#34; \\ \u0026amp;\u0026amp; mkdir -p /usr/src/redis \\ \u0026amp;\u0026amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \\ \u0026amp;\u0026amp; make -C /usr/src/redis \\ \u0026amp;\u0026amp; make -C /usr/src/redis install \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \\ \u0026amp;\u0026amp; rm redis.tar.gz \\ \u0026amp;\u0026amp; rm -r /usr/src/redis \\ \u0026amp;\u0026amp; apt-get purge -y --auto-remove $buildDeps 而不是一行一个RUN\n镜像上下文（context） 如果注意，会看到 docker build 命令最后有一个 .。. 表示当前目录\n为什么呢？？\n理解docker bulid的工作原理，我们所用的docker命令是客户端工具，表面上我们是使用docker命令来执行各种docker功能，但是实际上所有的远程调用形式都是在服务端（docker引擎）完成。C/S设计\n如何才能让服务端获得本地文件呢？？\n这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。\n比如\nCOPY ./package.json /app/ 这并不是要复制执行 docker build 命令所在的目录下的 package.json，也不是复制 Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的 package.json\nDockerfile 指令 COPY 复制文件 格式\nCOPY \u0026lt;源路径\u0026gt;\u0026hellip; \u0026lt;目标路径\u0026gt; COPY [\u0026quot;\u0026lt;源路径1\u0026gt;\u0026quot;,\u0026hellip; \u0026ldquo;\u0026lt;目标路径\u0026gt;\u0026rdquo;] COPY 指令将从构建上下文目录中 \u0026lt;源路径\u0026gt; 的文件/目录复制到新的一层的镜像内的 \u0026lt;目标路径\u0026gt; 位置。比如：\nCOPY package.json /usr/src/app/ ADD 更高级的复制文件 格式和COPY差不多\n这个只适合自解压的场合\n如果 \u0026lt;源路径\u0026gt; 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 \u0026lt;目标路径\u0026gt; 去。\nFROM scratch ADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz / ... 这样就会自动解压到根目录\nCMD容器启动命令 CMD 指令就是用于指定默认的容器主进程的启动命令的。\n在运行时可以指定新的命令来替代镜像设置中的这个默认命令，比如，ubuntu 镜像默认的 CMD 是 /bin/shell，如果我们直接 docker run -it ubuntu 的话，会直接进入 shell。我们也可以在运行时指定运行别的命令，如 docker run -it ubuntu cat /etc/os-release。这就是用 cat /etc/os-release 命令替换了默认的 /bin/shell 命令了，输出了系统版本信息。\nDocker容器不是虚拟机，所有的应用都应该前台运行\n容器内没有后台的概念\nCMD、RUN、ENTRYPOINT概念理解可以看docker心得①\nshell模式并不好用。推荐使用exec模式\nENTRYPOINT入口 可以把镜像当命令使用 跟在镜像后的是command，会替换CMD的默认值（写在dockerfile里面的cmd值）\n如果都是使用CMD，我们在运行需要带参数的镜像就不行了会报错\nCMD [\u0026#34;curl\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;http://ip.gs\u0026#34;] 这样就无法就后面接上参数\n报错，因为-i不是一个CMD命令\n$ docker run ip -i 如果是使用ENTRYPOINT，CMD的内容会将会作为参数传给ENTRYPOINT，比如\n$ docker run ip -i -i将会作为新的CMD传回来\n应用运行前的准备工作 启动了容器就是启动主进程，但是我们在这之前需要准备一下其他的工作\n可以写一个脚本判断CMD后面的命令来选择执行某些工作\n比如启动一个redis容器的时候，如果不是redis-server命令启动，我们就会使用root用户登录\n$ docker run -it redis id uid=0(root) gid =0(root) groups =0 (root) ###未完待续~\n","date":"2017-08-16T11:42:55Z","permalink":"https://kinokosu3.github.io/p/docker%E5%AD%A6%E4%B9%A0%E7%BA%AA%E5%BD%95/","title":"Docker学习纪录②"},{"content":" docker中RUN、CMD、ENTRTPOINT的区别 一开始我是对这三个理解的不太够透彻，因为这三个都是运行shell下的命令\n最重要的是他们的运行时机不一样，RUN是bulid镜像是用到的指令，最终会commit到镜像\nCMD和ENTRTPOINT则是运行时候使用，运行镜像时就会运行这些命令\n","date":"2017-08-10T19:02:45Z","permalink":"https://kinokosu3.github.io/p/docker%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97/","title":"Docker学习心得①"},{"content":" 因为自己的代码写的太难看，不太敢老是丢在github上，但是我又想有一个远程仓库，然后手上有一个树莓派，我们就可以用这个来搭建一个git服务器\n适合Ubuntu和debian\n安装Git $ sudo apt-get install git 建立一个git用户 我们以后要用推送到远程仓库，需要使用这个账户使用git服务\n$ sudo useradd git 生成公钥 在用户文件夹路径下（/home/git/）输入命令\n$ ssh-keygen –t rsa 一路回车\n然后命令ls -a 查看是否有.ssh文件夹，进入\n配置无密码ssh登录 在/home/git/.ssh输入命令\n$ sudo vim authorized_keys 你会发现有两个文件id_rsa.pub和id_rsa，id_rsa.pub是公钥文件\n创建authorized_keys文件，然后将你的登录的机器的公钥放进去，你的公钥一样在.ssh文件夹的id_rsa.pub里\n然后我们重启试试能不能用git用户无密码登录\n初始化git仓库 先选定一个目录作为git仓库，然后创建一个名为sample.git的git仓库\n$ sudo git init --bare sample.git 在git里，\u0026ndash;bare是创建一个裸仓库，服务器上的git仓库都以git结尾，然后我们修改权限\n$ sudo chown -R git:git sample.git 推送和克隆 如果我们要推送一个仓库到这个裸仓库，或者以后需要远程推送，我们就需要使用push命令\n如果是第一次推送，首先我们要关联一个远程库，也就是之前的裸仓库\n$ git remote add origin git@地址:git仓库路径 添加后，远程库的名字就是origin，这是Git默认的叫法，也可以改成别的，但是origin这个名字一看就知道是远程库。\n然后我们就可以把本地库的内容推送的远程库上：\n$ git push -u origin master 使用git push命令，其实是把当前分支master推送到远程上\n由于我们这是第一次推送仓库，远程库是空的，所以我们加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来\n如果我们本体做了修改，我们就可以通过命令\n$ git push origin master 把本地master分支的最新修改推送到github\n","date":"2017-08-10T10:26:52Z","permalink":"https://kinokosu3.github.io/p/%E6%90%AD%E5%BB%BAgit%E6%9C%8D%E5%8A%A1%E5%99%A8/","title":"搭建git服务器"},{"content":" 干嘛要用docker 一种新兴的虚拟化方式\n更高效的运用系统资源 容器不需要硬件虚拟以及运行完整操作系统\n更快的启动 直接运行于宿主内核\n一致的运行环境 这句话有毒。。。确保了应用运行环境一致性，从而不会再出现 “这段代码在我机器上没问题啊” 这类问题。\n基本概念 镜像（image） 容器（container） 仓库（repository） 获取镜像 $ docker pull [选项] [Docker Registry地址]\u0026lt;仓库名\u0026gt;:\u0026lt;标签\u0026gt; 列出镜像 $ docker images 镜像ID是镜像的唯一标识，一个镜像可以对应多个标签\n虚悬镜像 中间层镜像 $ docker images -a 这样我们会看到很多无标签的镜像，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。\n列出部分镜像 $ docker images ubuntu REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 16.04 f753707788c5 4 weeks ago 127 MB ubuntu latest f753707788c5 4 weeks ago 127 MB ubuntu 14.04 1e0c3dd64ccd 4 weeks ago 188 MB docker images 还支持filter过滤器\n特定格式列出镜像 $ docker images -q 以上都是杂谈，docker file是docker的核心","date":"2017-08-07T15:51:54Z","permalink":"https://kinokosu3.github.io/p/docker%E5%AD%A6%E4%B9%A0%E7%BA%AA%E5%BD%95/","title":"Docker学习纪录①"},{"content":" 一些命令 显示当前内存分配给显卡多少 pi@raspberrypi:~ $ vcgencmd get_mem gpu gpu=64M 更改分配sudo vim /boot/config.txt\n在末尾添加gpu_ mem=16\n","date":"2017-08-07T15:35:51Z","permalink":"https://kinokosu3.github.io/p/%E6%A0%91%E8%8E%93%E6%B4%BE3b%E6%8A%98%E8%85%BE%E7%BA%AA%E5%BD%95/","title":"树莓派3B折腾纪录③"},{"content":" 安装好系统之后的一点折腾 更改ssh密码 登录上之后有个警告,告诉我们需要更改默认密码\nSSH is enabled and the default password for the \u0026#39;pi\u0026#39; user has not been changed. This is a security risk - please login as the \u0026#39;pi\u0026#39; user and type \u0026#39;passwd\u0026#39; to set a new password. 我们输入\npassswd 根据提示输入当前密码，新密码就好了\n在哪个用户下输入passwd就更改哪个用户的密码，sudo passwd更改的是root的密码\n安装python3 RASPBIAN lite版系统自带只有python2，没有python3\npython3 -shell: python3: command not found 我们下载py3的包\nwget https://www.python.org/ftp/python/3.6.2/Python-3.6.2.tar.xz 我们使用wget命令来下载，下载路径是当前路径，py3的版本可以自己选择\n由于大家都知道的原因，速度会极慢。。。等等就好\n输入命令解压\ntar -xvf Python-3.6.2.tar.xz tar是liunx是压缩命令，-x是解压缩或打包的功能, -v显示正在处理的文件名，-f后面接要被处理的文件名\n然后cd进解压出来的文件夹里，README.rst是教程，(先偷偷安装一个vim)\n教程提示我们\nOn Unix, Linux, BSD, macOS, and Cygwin:: ./configure make make test sudo make install 但是！！但是！！先不要编译！！！\n因为这样直接编译，pip安装第三方库的时候会出现ssl错误，所以我们要先改动一些配置文件\n先输入命令 sudo apt-get install libssl-dev安装\n然后我们进入刚刚解压出来的python文件夹，使用vim Modules/Setup打开文件，把这几句的注释去掉（文件默认前面是带有#号的）\n_socket socketmodule.c timemodule.c _ssl _ssl.c \\ -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \\ -L$(SSL)/lib -lssl -lcrypto 然后我们才开始编译（又是一段可怕的时光，所以我想用docker。。。。）\n","date":"2017-08-07T14:41:44Z","permalink":"https://kinokosu3.github.io/p/%E6%A0%91%E8%8E%93%E6%B4%BE3b%E6%8A%98%E8%85%BE%E7%BA%AA%E5%BD%95/","title":"树莓派3B折腾纪录②"},{"content":"树莓派系统的安装 系统选择 我的树莓派3B是在某宝上买的，买的是没有带系统的版本（没有买带有TF卡套餐的），就需要自己装一个系统，折腾树莓派以后自己装系统应该会成为常态的\n官网的下载地址: 地址\nNOOBS是官方提供的安装器，左边的完整版自带离线安装包RASPBIAN，可以通过NOOBS安装器安装，右边的lite版是线上安装模式，需要通过从网上下载镜像来安装。NOOBS适合不习惯命令行的小白\nRASPBIAN是刷入版，对于NOOBS的区别大概就是一个完整的镜像烧入卡里面，左边是完整的带有图形环境的完整桌面版，带有各种软件，右边的lite版是阉割了图形环境和一些软件的命令行版（一般不用图形界面的就用lite版）\n安装方式 得到一张TF卡，首先要把它格式化，使用SDFormatter格式化\nNOOBS安装 把下载后的压缩包解压出来的文件全部复制进刚刚格式化成FAT32的TF卡里就可以了\nRASPBIAN安装 使用win32diskimager烧入镜像\n登录树莓派 图形化界面没有可以说的，跟平常开电脑是一样的，插上HDMI线就可以了\n接下来介绍不用显示器登录树莓派的方法\nssh登录 无论是NOOBS安装的系统，还是烧入的系统，在TF卡根目录建一个ssh的空文件（不要有任何后缀）\n因为不知道什么时候的事，官方的系统默认关闭了ssh功能，我们需要这样开启\n把树莓派的电源插上之后，等大概20秒的时间，这时候我们不知道树莓派的ip，我们可以进路由器后台查看，或者使用 扫描器扫描一下局域网的网络设备，还有一个方法就是用网线插上笔记本，然后共享网络，进入CMD使用命令arp -a查看树莓派在当前路由的地址，如果你不对路由器断电，这个地址会绑定MAC地址不会变的。\n接下来我们可以ssh登录了，在windows环境下可以使用xshell或者puttyport进行ssh登录\n我使用mac环境（liunx同理）\n在终端输入\n$ ssh pi@192.168.0.106 192.168.0.106是我树莓派在路由器的地址\n然后会有交互显示\nThe authenticity of host \u0026#39;192.168.0.106 (192.168.0.106)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:7gNwDQEdPve2Cjk6eDqQhxvi5X/ZqDRSVKpv38T7Gr8. Are you sure you want to continue connecting (yes/no)? yes 输入yes就好了\n如果你重新刷了系统，然后树莓派的地址和之前的一样，就会触发Host key verification failed错误,mac环境下是删除.shh里面know_hosts文件的ip密钥纪录\n之后会出现输入密码\npi@192.168.0.106\u0026#39;s password: 默认密码是raspberry\n到此，我们就使用ssh登录上了树莓派\n","date":"2017-08-06T14:41:44Z","permalink":"https://kinokosu3.github.io/p/%E6%A0%91%E8%8E%93%E6%B4%BE3b%E6%8A%98%E8%85%BE%E7%BA%AA%E5%BD%95/","title":"树莓派3B折腾纪录①"}]