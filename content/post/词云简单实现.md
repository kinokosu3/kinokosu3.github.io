---
title: 词云简单实现
date: 2018-03-16 16:49:14
tags:
    - WordCloud
    - 中文分词
---

## 前言

对于数据挖掘来说，并不都是巨量的数据集和复杂而抽象的数学模型，有的时候，我们也需要直观的图形来表示，对于文本的数据分析来说，词云`wordcloud`就是其中一种较为流行的可视化方式。下图就是[Solidot](https://www.solidot.org/)3个月新闻的高频词的一个可视化。
<!--more-->

## 实例展示

### TOP10

{%asset_img 10.png test%}

### TOP50

{%asset_img 50.png test%}

### TOP100

{%asset_img 100.png test%}

## 中文分词简单实现

经过对比几个流行的python中文分词库，我选择了[jieba](https://github.com/fxsjy/jieba)

### jieba主要模式

支持三种分词模式：

* 精确模式，试图将句子最精确地切开，适合文本分析；
* 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
* 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。支持繁体分词
  支持自定义词典

其他的功能可以进入该库的github页面学习[jieba](https://github.com/fxsjy/jieba)。

### TF-IDF算法的关键词的提取

关于该算法的解释: [TF-IDF](https://zh.wikipedia.org/wiki/Tf-idf)

因为`jieba`集成了实现，所以我们可以很简单的使用这个算法进行高频词提取。

python的使用:

```python
import jieba.analyse

# text_from_file是待提取的文本 
# tok提取50个高频词(前top50)
# withWeight 为是否一并返回关键词权重值，默认值为 False
# tags为一个元组元素组成的列表，元素为关键词和关键词权重值。
tags = jieba.analyse.extract_tags(text_from_file, topK=50,  withWeight=True)
```

### 停用词和词典的导入

如果你不是使用停用词和自定义的词典的话，你会发现像`万一`、`亿美元`之类的词会成为我们的高频词，一篇文章自然会出现这些量词之类的，而且会频繁出现，或者会把如`云计算`拆分成`计算`这个词，而我们需要的是里面出现的一些名词，这时候我们就需要停用词和自定义词典来更为准确的分词。

```python
jieba.load_userdict("userdict.txt")
jieba.analyse.set_stop_words('stop_words.txt')
```

停用词词典示例:

{%asset_img stop_word.png stop_word%}

## WordCloud(词云)构建

### 主要参数

```python
# from wordcloud import WordCloud
# font_path为字体
# random_state 随机数种子


wc = WordCloud(
        font_path='simkai.ttf',
        background_color='white',
        max_words=50,
        max_font_size=200,
        random_state=42,
        width=1500, height=1500,
)
```

需要注意的是`WordCloud`如果不指定中文字体会无法显示

`wc.generate_from_frequencies(data_dict)`根据词频生成`WordCloud`。参数为Python的字典类型。

如果想使用`matplotlib`进行调试的话，我们这样使用:

```python
import matplotlib.pyplot as plt
plt.figure()
plt.imshow(wc)
plt.axis("off")
plt.show()
```

> matplotlib显示的时候会比输出的图片有白边，是因为那个地方原来是坐标系，我们使用axis()方法屏蔽了

### 保存图片

```python
wc.to_file('test.png')
```

### 词云形状

其实还可以使用图片来生成词云的形状的，这个可以百度或者谷歌一下