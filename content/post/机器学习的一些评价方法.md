---
title: 机器学习的一些评价方法
date: 2018-06-07 19:09:00
tags:
    - 机器学习
    - TensorFlow
    - python
---

## tensorflow里的loss function

### cross_entropy交叉熵

交叉熵刻画的是两个概率分布之间的距离，我们通过softmax回归将神经网络前向传播得到的结果变成交叉熵要求的概率分布得分。

`softmax`函数用于多分类神经网络输出, 使得神经网络输出层输出的是一个概率分布，哪个点的概率高代表着某个类别的概率高。
`sigmoid`函数也用于隐层输出，使用哪个函数看业务需求
<!-- more -->

#### `sigmoid_cross_entropy_with_logits`

这个函数的输入是`logits`和`targets`，`logits`就是神经网络模型中的 W * X矩阵，注意不需要经过`sigmoid`，而`targets`的`shape`和`logits`相同，就是正确的`label`值，例如这个模型一次要判断100张图是否包含10种动物，这两个输入的shape都是[100, 10]。注释中还提到这10个分类之间是独立的、不要求是互斥，这种问题我们成为多目标，例如判断图片中是否包含10种动物，label值可以包含多个1或0个1，还有一种问题是多分类问题，例如我们对年龄特征分为5段，只允许5个值有且只有1个值为1，这种问题可以直接用这个函数吗？答案是不可以。

-- 引用[TensorFlow四种Cross Entropy算法实现和应用](https://blog.csdn.net/heyc861221/article/details/80127148)

> 对于多分类问题, 我们可以使用softmax函数。

#### `softmax_cross_entropy_with_logits`

这个函数只适合单目标的二分类或者多分类问题。

对于多分类问题，例如我们的年龄分为5类，并且人工编码为0、1、2、3、4，因为输出值是5维的特征，因此我们需要人工做onehot encoding分别编码为00001、00010、00100、01000、10000，才可以作为这个函数的输入。
也就是说，原数据集的`label`需要做`one-hot`处理才可以使用这个函数。

具体的执行流程大概分为两步，第一步首先是对网络最后一层的输出做一个softmax。

这里需要注意的是，这个函数返回值不是一个数，而是一个向量，如果要求交叉熵，我们要在做一步`tf.resuce_sum`操作，就是对向量里面的所有元素求和, 最后就能得到Hy′(y),如果要求loss，则需要做一步`tf.reduce_mean`操作，对向量求均值.

warning：

* Tenosrflow中集成的交叉熵操作是施加在未经过`Softmax`处理的`logits`上, 这个操作的输入`logits`是未经缩放的, 该操作内部会对`logits`使用`Softmax`操作。

#### `tf.nn.sparse_softmax_cross_entropy_with_logits`

`softmax_cross_entropy_with_logits`的易用版本`

该函数与`tf.nn.softmax_cross_entropy_with_logits()`函数十分相似，唯一的区别在于`labels`的`shape`，该函数的`labels`要求是排他性的即只有一个正确的类别，如果`labels`的每一行不需要进行`one_hot`表示，可以使用`tf.nn.sparse_softmax_cross_entropy_with_logits()`。

